{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import datetime as dt\n",
    "import torch as torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch.nn as nn \n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, MaxAbsScaler, RobustScaler\n",
    "\n",
    "from scipy.special import softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('clean_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Brazil Open</th>\n",
       "      <th>Brazil High</th>\n",
       "      <th>Brazil Low</th>\n",
       "      <th>Brazil Close</th>\n",
       "      <th>Brazil Adj Close</th>\n",
       "      <th>Brazil Volume</th>\n",
       "      <th>Brazil Returns</th>\n",
       "      <th>Russia Open</th>\n",
       "      <th>Russia High</th>\n",
       "      <th>...</th>\n",
       "      <th>South Africa Returns</th>\n",
       "      <th>year</th>\n",
       "      <th>sin_day</th>\n",
       "      <th>cos_day</th>\n",
       "      <th>sin_day_of_week</th>\n",
       "      <th>cos_day_of_week</th>\n",
       "      <th>sin_month</th>\n",
       "      <th>cos_month</th>\n",
       "      <th>sin_week_of_year</th>\n",
       "      <th>cos_week_of_year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>78.500000</td>\n",
       "      <td>79.370003</td>\n",
       "      <td>78.070000</td>\n",
       "      <td>79.220001</td>\n",
       "      <td>53.268791</td>\n",
       "      <td>1.606910e+07</td>\n",
       "      <td>0.022062</td>\n",
       "      <td>149.070007</td>\n",
       "      <td>150.550003</td>\n",
       "      <td>...</td>\n",
       "      <td>0.028368</td>\n",
       "      <td>2011</td>\n",
       "      <td>0.258819</td>\n",
       "      <td>0.965926</td>\n",
       "      <td>-0.433884</td>\n",
       "      <td>-0.900969</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.123234e-17</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-1.608123e-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>78.846667</td>\n",
       "      <td>79.520002</td>\n",
       "      <td>78.390000</td>\n",
       "      <td>79.406667</td>\n",
       "      <td>53.394311</td>\n",
       "      <td>1.444657e+07</td>\n",
       "      <td>0.002356</td>\n",
       "      <td>150.013336</td>\n",
       "      <td>151.363337</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001658</td>\n",
       "      <td>2011</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>-0.974928</td>\n",
       "      <td>-0.222521</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.123234e-17</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-1.608123e-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>79.193334</td>\n",
       "      <td>79.670001</td>\n",
       "      <td>78.709999</td>\n",
       "      <td>79.593333</td>\n",
       "      <td>53.519831</td>\n",
       "      <td>1.282403e+07</td>\n",
       "      <td>0.002351</td>\n",
       "      <td>150.956665</td>\n",
       "      <td>152.176671</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001655</td>\n",
       "      <td>2011</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>-0.781831</td>\n",
       "      <td>0.623490</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.123234e-17</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-1.608123e-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>79.540001</td>\n",
       "      <td>79.820000</td>\n",
       "      <td>79.029999</td>\n",
       "      <td>79.779999</td>\n",
       "      <td>53.645351</td>\n",
       "      <td>1.120150e+07</td>\n",
       "      <td>0.002345</td>\n",
       "      <td>151.899994</td>\n",
       "      <td>152.990005</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001652</td>\n",
       "      <td>2011</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.123234e-17</td>\n",
       "      <td>0.992709</td>\n",
       "      <td>-1.205367e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>79.449997</td>\n",
       "      <td>80.080002</td>\n",
       "      <td>79.230003</td>\n",
       "      <td>79.510002</td>\n",
       "      <td>53.463795</td>\n",
       "      <td>1.189970e+07</td>\n",
       "      <td>-0.003384</td>\n",
       "      <td>151.160004</td>\n",
       "      <td>152.160004</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000786</td>\n",
       "      <td>2011</td>\n",
       "      <td>0.965926</td>\n",
       "      <td>0.258819</td>\n",
       "      <td>0.781831</td>\n",
       "      <td>0.623490</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.123234e-17</td>\n",
       "      <td>0.992709</td>\n",
       "      <td>-1.205367e-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 45 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  Brazil Open  Brazil High  Brazil Low  Brazil Close  \\\n",
       "0           0    78.500000    79.370003   78.070000     79.220001   \n",
       "1           1    78.846667    79.520002   78.390000     79.406667   \n",
       "2           2    79.193334    79.670001   78.709999     79.593333   \n",
       "3           3    79.540001    79.820000   79.029999     79.779999   \n",
       "4           4    79.449997    80.080002   79.230003     79.510002   \n",
       "\n",
       "   Brazil Adj Close  Brazil Volume  Brazil Returns  Russia Open  Russia High  \\\n",
       "0         53.268791   1.606910e+07        0.022062   149.070007   150.550003   \n",
       "1         53.394311   1.444657e+07        0.002356   150.013336   151.363337   \n",
       "2         53.519831   1.282403e+07        0.002351   150.956665   152.176671   \n",
       "3         53.645351   1.120150e+07        0.002345   151.899994   152.990005   \n",
       "4         53.463795   1.189970e+07       -0.003384   151.160004   152.160004   \n",
       "\n",
       "   ...  South Africa Returns  year   sin_day   cos_day  sin_day_of_week  \\\n",
       "0  ...              0.028368  2011  0.258819  0.965926        -0.433884   \n",
       "1  ...              0.001658  2011  0.500000  0.866025        -0.974928   \n",
       "2  ...              0.001655  2011  0.707107  0.707107        -0.781831   \n",
       "3  ...              0.001652  2011  0.866025  0.500000         0.000000   \n",
       "4  ...              0.000786  2011  0.965926  0.258819         0.781831   \n",
       "\n",
       "   cos_day_of_week  sin_month     cos_month  sin_week_of_year  \\\n",
       "0        -0.900969        1.0  6.123234e-17          1.000000   \n",
       "1        -0.222521        1.0  6.123234e-17          1.000000   \n",
       "2         0.623490        1.0  6.123234e-17          1.000000   \n",
       "3         1.000000        1.0  6.123234e-17          0.992709   \n",
       "4         0.623490        1.0  6.123234e-17          0.992709   \n",
       "\n",
       "   cos_week_of_year  \n",
       "0     -1.608123e-16  \n",
       "1     -1.608123e-16  \n",
       "2     -1.608123e-16  \n",
       "3     -1.205367e-01  \n",
       "4     -1.205367e-01  \n",
       "\n",
       "[5 rows x 45 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns={'Unnamed: 0'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Brazil Open</th>\n",
       "      <th>Brazil High</th>\n",
       "      <th>Brazil Low</th>\n",
       "      <th>Brazil Close</th>\n",
       "      <th>Brazil Adj Close</th>\n",
       "      <th>Brazil Volume</th>\n",
       "      <th>Brazil Returns</th>\n",
       "      <th>Russia Open</th>\n",
       "      <th>Russia High</th>\n",
       "      <th>Russia Low</th>\n",
       "      <th>...</th>\n",
       "      <th>South Africa Returns</th>\n",
       "      <th>year</th>\n",
       "      <th>sin_day</th>\n",
       "      <th>cos_day</th>\n",
       "      <th>sin_day_of_week</th>\n",
       "      <th>cos_day_of_week</th>\n",
       "      <th>sin_month</th>\n",
       "      <th>cos_month</th>\n",
       "      <th>sin_week_of_year</th>\n",
       "      <th>cos_week_of_year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>78.500000</td>\n",
       "      <td>79.370003</td>\n",
       "      <td>78.070000</td>\n",
       "      <td>79.220001</td>\n",
       "      <td>53.268791</td>\n",
       "      <td>1.606910e+07</td>\n",
       "      <td>0.022062</td>\n",
       "      <td>149.070007</td>\n",
       "      <td>150.550003</td>\n",
       "      <td>149.070007</td>\n",
       "      <td>...</td>\n",
       "      <td>0.028368</td>\n",
       "      <td>2011</td>\n",
       "      <td>0.258819</td>\n",
       "      <td>0.965926</td>\n",
       "      <td>-0.433884</td>\n",
       "      <td>-0.900969</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.123234e-17</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-1.608123e-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>78.846667</td>\n",
       "      <td>79.520002</td>\n",
       "      <td>78.390000</td>\n",
       "      <td>79.406667</td>\n",
       "      <td>53.394311</td>\n",
       "      <td>1.444657e+07</td>\n",
       "      <td>0.002356</td>\n",
       "      <td>150.013336</td>\n",
       "      <td>151.363337</td>\n",
       "      <td>150.013336</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001658</td>\n",
       "      <td>2011</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>-0.974928</td>\n",
       "      <td>-0.222521</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.123234e-17</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-1.608123e-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>79.193334</td>\n",
       "      <td>79.670001</td>\n",
       "      <td>78.709999</td>\n",
       "      <td>79.593333</td>\n",
       "      <td>53.519831</td>\n",
       "      <td>1.282403e+07</td>\n",
       "      <td>0.002351</td>\n",
       "      <td>150.956665</td>\n",
       "      <td>152.176671</td>\n",
       "      <td>150.956665</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001655</td>\n",
       "      <td>2011</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>-0.781831</td>\n",
       "      <td>0.623490</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.123234e-17</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-1.608123e-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>79.540001</td>\n",
       "      <td>79.820000</td>\n",
       "      <td>79.029999</td>\n",
       "      <td>79.779999</td>\n",
       "      <td>53.645351</td>\n",
       "      <td>1.120150e+07</td>\n",
       "      <td>0.002345</td>\n",
       "      <td>151.899994</td>\n",
       "      <td>152.990005</td>\n",
       "      <td>151.899994</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001652</td>\n",
       "      <td>2011</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.123234e-17</td>\n",
       "      <td>0.992709</td>\n",
       "      <td>-1.205367e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>79.449997</td>\n",
       "      <td>80.080002</td>\n",
       "      <td>79.230003</td>\n",
       "      <td>79.510002</td>\n",
       "      <td>53.463795</td>\n",
       "      <td>1.189970e+07</td>\n",
       "      <td>-0.003384</td>\n",
       "      <td>151.160004</td>\n",
       "      <td>152.160004</td>\n",
       "      <td>151.160004</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000786</td>\n",
       "      <td>2011</td>\n",
       "      <td>0.965926</td>\n",
       "      <td>0.258819</td>\n",
       "      <td>0.781831</td>\n",
       "      <td>0.623490</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.123234e-17</td>\n",
       "      <td>0.992709</td>\n",
       "      <td>-1.205367e-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 44 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Brazil Open  Brazil High  Brazil Low  Brazil Close  Brazil Adj Close  \\\n",
       "0    78.500000    79.370003   78.070000     79.220001         53.268791   \n",
       "1    78.846667    79.520002   78.390000     79.406667         53.394311   \n",
       "2    79.193334    79.670001   78.709999     79.593333         53.519831   \n",
       "3    79.540001    79.820000   79.029999     79.779999         53.645351   \n",
       "4    79.449997    80.080002   79.230003     79.510002         53.463795   \n",
       "\n",
       "   Brazil Volume  Brazil Returns  Russia Open  Russia High  Russia Low  ...  \\\n",
       "0   1.606910e+07        0.022062   149.070007   150.550003  149.070007  ...   \n",
       "1   1.444657e+07        0.002356   150.013336   151.363337  150.013336  ...   \n",
       "2   1.282403e+07        0.002351   150.956665   152.176671  150.956665  ...   \n",
       "3   1.120150e+07        0.002345   151.899994   152.990005  151.899994  ...   \n",
       "4   1.189970e+07       -0.003384   151.160004   152.160004  151.160004  ...   \n",
       "\n",
       "   South Africa Returns  year   sin_day   cos_day  sin_day_of_week  \\\n",
       "0              0.028368  2011  0.258819  0.965926        -0.433884   \n",
       "1              0.001658  2011  0.500000  0.866025        -0.974928   \n",
       "2              0.001655  2011  0.707107  0.707107        -0.781831   \n",
       "3              0.001652  2011  0.866025  0.500000         0.000000   \n",
       "4              0.000786  2011  0.965926  0.258819         0.781831   \n",
       "\n",
       "   cos_day_of_week  sin_month     cos_month  sin_week_of_year  \\\n",
       "0        -0.900969        1.0  6.123234e-17          1.000000   \n",
       "1        -0.222521        1.0  6.123234e-17          1.000000   \n",
       "2         0.623490        1.0  6.123234e-17          1.000000   \n",
       "3         1.000000        1.0  6.123234e-17          0.992709   \n",
       "4         0.623490        1.0  6.123234e-17          0.992709   \n",
       "\n",
       "   cos_week_of_year  \n",
       "0     -1.608123e-16  \n",
       "1     -1.608123e-16  \n",
       "2     -1.608123e-16  \n",
       "3     -1.205367e-01  \n",
       "4     -1.205367e-01  \n",
       "\n",
       "[5 rows x 44 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_label_split(df, target_cols):\n",
    "    targets = [target for target in target_cols]\n",
    "    y = df[targets]\n",
    "    X = df.drop(columns=targets)\n",
    "    return X, y\n",
    "\n",
    "def train_val_test_split(df, target_cols, test_ratio):\n",
    "    val_ratio = test_ratio / (1 - test_ratio)\n",
    "    X, y = feature_label_split(df, target_cols)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_ratio, shuffle=False)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=val_ratio, shuffle=False)\n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test\n",
    "\n",
    "returns = [returns for returns in df.columns if 'Returns' in returns] # unreal stuff, don't ask me how \n",
    "X_train, X_val, X_test, y_train, y_val, y_test = train_val_test_split(df, returns, 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Brazil Returns</th>\n",
       "      <th>Russia Returns</th>\n",
       "      <th>India Returns</th>\n",
       "      <th>China Returns</th>\n",
       "      <th>South Africa Returns</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.022062</td>\n",
       "      <td>0.018675</td>\n",
       "      <td>0.000995</td>\n",
       "      <td>0.013873</td>\n",
       "      <td>0.028368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.002356</td>\n",
       "      <td>0.004362</td>\n",
       "      <td>0.000532</td>\n",
       "      <td>0.007095</td>\n",
       "      <td>0.001658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.002351</td>\n",
       "      <td>0.004343</td>\n",
       "      <td>0.000532</td>\n",
       "      <td>0.007045</td>\n",
       "      <td>0.001655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.002345</td>\n",
       "      <td>0.004324</td>\n",
       "      <td>0.000532</td>\n",
       "      <td>0.006996</td>\n",
       "      <td>0.001652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.003384</td>\n",
       "      <td>-0.002360</td>\n",
       "      <td>-0.003597</td>\n",
       "      <td>-0.005955</td>\n",
       "      <td>0.000786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2404</th>\n",
       "      <td>-0.011406</td>\n",
       "      <td>0.004094</td>\n",
       "      <td>0.004940</td>\n",
       "      <td>-0.002571</td>\n",
       "      <td>0.002153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2405</th>\n",
       "      <td>0.000249</td>\n",
       "      <td>-0.012957</td>\n",
       "      <td>-0.003214</td>\n",
       "      <td>0.009742</td>\n",
       "      <td>0.004382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2406</th>\n",
       "      <td>-0.006721</td>\n",
       "      <td>0.012812</td>\n",
       "      <td>-0.020878</td>\n",
       "      <td>0.006891</td>\n",
       "      <td>0.003056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2407</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.006118</td>\n",
       "      <td>-0.009533</td>\n",
       "      <td>0.001521</td>\n",
       "      <td>0.006476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2408</th>\n",
       "      <td>-0.014787</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.016247</td>\n",
       "      <td>-0.000304</td>\n",
       "      <td>-0.008408</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2409 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Brazil Returns  Russia Returns  India Returns  China Returns  \\\n",
       "0           0.022062        0.018675       0.000995       0.013873   \n",
       "1           0.002356        0.004362       0.000532       0.007095   \n",
       "2           0.002351        0.004343       0.000532       0.007045   \n",
       "3           0.002345        0.004324       0.000532       0.006996   \n",
       "4          -0.003384       -0.002360      -0.003597      -0.005955   \n",
       "...              ...             ...            ...            ...   \n",
       "2404       -0.011406        0.004094       0.004940      -0.002571   \n",
       "2405        0.000249       -0.012957      -0.003214       0.009742   \n",
       "2406       -0.006721        0.012812      -0.020878       0.006891   \n",
       "2407        0.000000       -0.006118      -0.009533       0.001521   \n",
       "2408       -0.014787        0.000000       0.016247      -0.000304   \n",
       "\n",
       "      South Africa Returns  \n",
       "0                 0.028368  \n",
       "1                 0.001658  \n",
       "2                 0.001655  \n",
       "3                 0.001652  \n",
       "4                 0.000786  \n",
       "...                    ...  \n",
       "2404              0.002153  \n",
       "2405              0.004382  \n",
       "2406              0.003056  \n",
       "2407              0.006476  \n",
       "2408             -0.008408  \n",
       "\n",
       "[2409 rows x 5 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "\n",
    "X_train_arr = scaler.fit_transform(X_train)\n",
    "X_val_arr = scaler.transform(X_val)\n",
    "X_test_arr = scaler.transform(X_test)\n",
    "\n",
    "y_train_arr = scaler.fit_transform(y_train)\n",
    "y_val_arr = scaler.transform(y_val)\n",
    "y_test_arr = scaler.transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "train_features = torch.Tensor(X_train_arr)\n",
    "train_targets = torch.Tensor(y_train_arr)\n",
    "val_features = torch.Tensor(X_val_arr)\n",
    "val_targets = torch.Tensor(y_val_arr)\n",
    "test_features = torch.Tensor(X_test_arr)\n",
    "test_targets = torch.Tensor(y_test_arr)\n",
    "\n",
    "train = TensorDataset(train_features, train_targets)\n",
    "val = TensorDataset(val_features, val_targets)\n",
    "test = TensorDataset(test_features, test_targets)\n",
    "\n",
    "train_loader = DataLoader(train, batch_size=batch_size, shuffle=False, drop_last=True)\n",
    "val_loader = DataLoader(val, batch_size=batch_size, shuffle=False, drop_last=True)\n",
    "test_loader = DataLoader(test, batch_size=batch_size, shuffle=False, drop_last=True)\n",
    "test_loader_one = DataLoader(test, batch_size=1, shuffle=False, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.7362, 0.4679, 0.4970, 0.6123, 0.6899],\n",
       "        [0.6579, 0.4117, 0.4943, 0.5690, 0.5185],\n",
       "        [0.6579, 0.4116, 0.4943, 0.5687, 0.5185],\n",
       "        ...,\n",
       "        [0.6218, 0.4448, 0.3708, 0.5677, 0.5275],\n",
       "        [0.6485, 0.3705, 0.4362, 0.5335, 0.5494],\n",
       "        [0.5898, 0.3945, 0.5851, 0.5218, 0.4539]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cpu', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, layer_dim, output_dim, dropout_prob):\n",
    "        super(LSTMModel, self).__init__()\n",
    "\n",
    "        # Defining the number of layers and the nodes in each layer\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.layer_dim = layer_dim\n",
    "\n",
    "        # LSTM layers\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_dim, hidden_dim, layer_dim, batch_first=True, dropout=dropout_prob\n",
    "        )\n",
    "\n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Initializing hidden state for first input with zeros\n",
    "        h0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).requires_grad_()\n",
    "\n",
    "        # Initializing cell state for first input with zeros\n",
    "        c0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).requires_grad_()\n",
    "\n",
    "        # We need to detach as we are doing truncated backpropagation through time (BPTT)\n",
    "        # If we don't, we'll backprop all the way to the start even after going through another batch\n",
    "        # Forward propagation by passing in the input, hidden state, and cell state into the model\n",
    "        out, (hn, cn) = self.lstm(x, (h0.detach(), c0.detach()))\n",
    "\n",
    "        # Reshaping the outputs in the shape of (batch_size, seq_length, hidden_size)\n",
    "        # so that it can fit into the fully connected layer\n",
    "        out = out[:, -1, :]\n",
    "\n",
    "        # Convert the final state to our desired output shape (batch_size, output_dim)\n",
    "        out = self.fc(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(model, model_params):\n",
    "    models = {\n",
    "        \"lstm\": LSTMModel,\n",
    "    }\n",
    "    return models.get(model.lower())(**model_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimization:\n",
    "    def __init__(self, model, loss_fn, optimizer):\n",
    "        self.model = model\n",
    "        self.loss_fn = loss_fn\n",
    "        self.optimizer = optimizer\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "    \n",
    "    def train_step(self, x, y):\n",
    "        # Sets model to train mode\n",
    "        self.model.train()\n",
    "\n",
    "        # Makes predictions\n",
    "        yhat = self.model(x)\n",
    "\n",
    "        # Computes loss\n",
    "        loss = self.loss_fn(y, yhat)\n",
    "\n",
    "        # Computes gradients\n",
    "        loss.backward()\n",
    "\n",
    "        # Updates parameters and zeroes gradients\n",
    "        self.optimizer.step()\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "        # Returns the loss\n",
    "        return loss.item()\n",
    "\n",
    "    def train(self, train_loader, val_loader, batch_size=64, n_epochs=50, n_features=1):\n",
    "        # model_path = f'models/{self.model}_{dt.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")}.pt'\n",
    "        model_path = 'models/model.pt'\n",
    "        \n",
    "        for epoch in range(1, n_epochs + 1):\n",
    "            batch_losses = []\n",
    "            for x_batch, y_batch in train_loader:\n",
    "                x_batch = x_batch.view([batch_size, -1, n_features]).to(device)\n",
    "                y_batch = y_batch.to(device)\n",
    "                loss = self.train_step(x_batch, y_batch)\n",
    "                batch_losses.append(loss)\n",
    "            training_loss = np.mean(batch_losses)\n",
    "            self.train_losses.append(training_loss)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                batch_val_losses = []\n",
    "                for x_val, y_val in val_loader:\n",
    "                    x_val = x_val.view([batch_size, -1, n_features]).to(device)\n",
    "                    y_val = y_val.to(device)\n",
    "                    self.model.eval()\n",
    "                    yhat = self.model(x_val)\n",
    "                    val_loss = self.loss_fn(y_val, yhat).item()\n",
    "                    batch_val_losses.append(val_loss)\n",
    "                validation_loss = np.mean(batch_val_losses)\n",
    "                self.val_losses.append(validation_loss)\n",
    "\n",
    "            if (epoch <= 10) | (epoch % 50 == 0):\n",
    "                print(\n",
    "                    f\"[{epoch}/{n_epochs}] Training loss: {training_loss:.4f}\\t Validation loss: {validation_loss:.4f}\"\n",
    "                )\n",
    "\n",
    "        torch.save(self.model.state_dict(), model_path)\n",
    "\n",
    "    def evaluate(self, test_loader, batch_size=1, n_features=1):\n",
    "\n",
    "        with torch.no_grad():\n",
    "            predictions = []\n",
    "            values = []\n",
    "            for x_test, y_test in test_loader:\n",
    "                x_test = x_test.view([batch_size, -1, n_features]).to(device)\n",
    "                y_test = y_test.to(device)\n",
    "                self.model.eval()\n",
    "                yhat = self.model(x_test)\n",
    "                predictions.append(yhat.to(device).detach().numpy())\n",
    "                values.append(y_test.to(device).detach().numpy())\n",
    "\n",
    "        return predictions, values\n",
    "\n",
    "    def plot_losses(self):\n",
    "        plt.plot(self.train_losses, label=\"Training loss\")\n",
    "        plt.plot(self.val_losses, label=\"Validation loss\")\n",
    "        plt.legend()\n",
    "        plt.title(\"Losses\")\n",
    "        plt.show()\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/100] Training loss: 0.2038\t Validation loss: 0.0600\n",
      "[2/100] Training loss: 0.0106\t Validation loss: 0.0175\n",
      "[3/100] Training loss: 0.0080\t Validation loss: 0.0116\n",
      "[4/100] Training loss: 0.0071\t Validation loss: 0.0104\n",
      "[5/100] Training loss: 0.0067\t Validation loss: 0.0088\n",
      "[6/100] Training loss: 0.0062\t Validation loss: 0.0078\n",
      "[7/100] Training loss: 0.0059\t Validation loss: 0.0071\n",
      "[8/100] Training loss: 0.0057\t Validation loss: 0.0064\n",
      "[9/100] Training loss: 0.0056\t Validation loss: 0.0059\n",
      "[10/100] Training loss: 0.0055\t Validation loss: 0.0056\n",
      "[50/100] Training loss: 0.0047\t Validation loss: 0.0045\n",
      "[100/100] Training loss: 0.0047\t Validation loss: 0.0045\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "input_dim = len(X_train.columns)\n",
    "output_dim = 5\n",
    "hidden_dim = 64\n",
    "layer_dim = 3\n",
    "batch_size = 64\n",
    "dropout = 0.2\n",
    "n_epochs = 100\n",
    "learning_rate = 1e-3\n",
    "weight_decay = 1e-6\n",
    "\n",
    "model_params = {'input_dim': input_dim,\n",
    "                'hidden_dim' : hidden_dim,\n",
    "                'layer_dim' : layer_dim,\n",
    "                'output_dim' : output_dim,\n",
    "                'dropout_prob' : dropout}\n",
    "\n",
    "model = get_model('lstm', model_params)\n",
    "\n",
    "loss_fn = nn.MSELoss(reduction=\"mean\")\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "opt = Optimization(model=model, loss_fn=loss_fn, optimizer=optimizer)\n",
    "opt.train(train_loader, val_loader, batch_size=batch_size, n_epochs=n_epochs, n_features=input_dim)\n",
    "opt.plot_losses()\n",
    "\n",
    "lstm_predictions, values = opt.evaluate(test_loader_one, batch_size=1, n_features=input_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "reg = xgb.XGBRegressor(\n",
    "    tree_method=\"hist\", \n",
    "    n_estimators=64)\n",
    "    \n",
    "clf = reg.fit(X_train_arr, y_train_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "xg_predictions = reg.predict(X_test_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.5851296 , 0.41117203, 0.47383857, 0.51966786, 0.47221434],\n",
       "       [0.60827315, 0.37263748, 0.4183382 , 0.59554946, 0.40028507],\n",
       "       [0.5982437 , 0.3779243 , 0.43675932, 0.58577263, 0.42589524],\n",
       "       ...,\n",
       "       [0.6800196 , 0.5518358 , 0.48310953, 0.6349142 , 0.52033705],\n",
       "       [0.6728498 , 0.5437957 , 0.48335737, 0.6618716 , 0.5423358 ],\n",
       "       [0.6658538 , 0.5452222 , 0.4622061 , 0.6377128 , 0.5501155 ]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xg_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch import nn, Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TESTING \n",
    "\n",
    "transformer_model = nn.Transformer(nhead=16, num_encoder_layers=12)\n",
    "src = torch.rand((32, 512, ))\n",
    "tgt = torch.rand((32, 512))\n",
    "out = transformer_model(src, tgt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2409, 39), (803, 5))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_arr.shape, y_test_arr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "\n",
    "\n",
    "def get_src_trg(\n",
    "        sequence: torch.Tensor, \n",
    "        enc_seq_len: int, \n",
    "        target_seq_len: int\n",
    "        ) -> Tuple[torch.tensor, torch.tensor, torch.tensor]:\n",
    "\n",
    "        \"\"\"\n",
    "        Generate the src (encoder input), trg (decoder input) and trg_y (the target)\n",
    "        sequences from a sequence. \n",
    "        Args:\n",
    "            sequence: tensor, a 1D tensor of length n where \n",
    "                    n = encoder input length + target sequence length  \n",
    "            enc_seq_len: int, the desired length of the input to the transformer encoder\n",
    "            target_seq_len: int, the desired length of the target sequence (the \n",
    "                            one against which the model output is compared)\n",
    "        Return: \n",
    "            src: tensor, 1D, used as input to the transformer model\n",
    "            trg: tensor, 1D, used as input to the transformer model\n",
    "            trg_y: tensor, 1D, the target sequence against which the model output\n",
    "                is compared when computing loss. \n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "        assert len(sequence) == enc_seq_len + target_seq_len, \"Sequence length does not equal (input length + target length)\"\n",
    "        \n",
    "        #print(\"From data.TransformerDataset.get_src_trg: sequence shape: {}\".format(sequence.shape))\n",
    "\n",
    "        # encoder input\n",
    "        src = sequence[:enc_seq_len] \n",
    "        \n",
    "        # decoder input. As per the paper, it must have the same dimension as the \n",
    "        # target sequence, and it must contain the last value of src, and all\n",
    "        # values of trg_y except the last (i.e. it must be shifted right by 1)\n",
    "        trg = sequence[enc_seq_len-1:len(sequence)-1]\n",
    "\n",
    "        # trg = trg[:, 0] \n",
    "\n",
    "        if len(trg.shape) == 1:\n",
    "            trg = trg.unsqueeze(-1)\n",
    "        \n",
    "        assert len(trg) == target_seq_len, \"Length of trg does not match target sequence length\"\n",
    "\n",
    "        # The target sequence against which the model output will be compared to compute loss\n",
    "        trg_y = sequence[-target_seq_len:,-5:]\n",
    "\n",
    "        #print(\"From data.TransformerDataset.get_src_trg: trg_y shape before slice: {}\".format(trg_y.shape))\n",
    "\n",
    "        # We only want trg_y to consist of the target variable not any potential exogenous variables\n",
    "        # trg_y = trg_y[:, 0]\n",
    "\n",
    "        assert len(trg_y) == target_seq_len, \"Length of trg_y does not match target sequence length\"\n",
    "\n",
    "        return src, trg, trg_y # change size from [batch_size, target_seq_len, num_features] to [batch_size, target_seq_len] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2409, 39), (803, 39))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_arr.shape, X_test_arr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = np.hstack((X_train_arr, y_train_arr))\n",
    "test_data = np.hstack((X_test_arr, y_test_arr))\n",
    "\n",
    "\n",
    "sequence = np.vstack((train_data, test_data))\n",
    "sequence = torch.tensor(sequence, dtype=torch.float32)\n",
    "\n",
    "enc_seq_len = len(y_train_arr)\n",
    "target_seq_len = len(y_test_arr)\n",
    "\n",
    "src, trg, trg_y = get_src_trg(sequence, enc_seq_len, target_seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2409, 44]), torch.Size([803, 44]), torch.Size([803, 5]))"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src.shape, trg.shape, trg_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 5)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src.size(-1), trg.size(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_model = nn.Transformer(d_model = 44, nhead = 11, num_encoder_layers=12)\n",
    "\n",
    "out = transformer_model(src, trg)\n",
    "transformer_predictions = out.detach().numpy()[:, -5:] # we just need the final 5 cols for the returns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformer_model = nn.Transformer(d_model = 44, nhead=11, num_encoder_layers=12)\n",
    "# src = torch.rand((2409, 44))\n",
    "# tgt = torch.rand((803, 44))\n",
    "# out = transformer_model(src, tgt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def depict_results(predictions):\n",
    "    \n",
    "    arr = []\n",
    "    for prediction in predictions:\n",
    "        arr.append(prediction.flatten())\n",
    "\n",
    "    predictions = np.array(arr)\n",
    "    predicted = scaler.inverse_transform(predictions) \n",
    "\n",
    "    df_predicted = pd.DataFrame(predicted, \n",
    "                columns=['Brazil Returns',\n",
    "                'Russia Returns',\n",
    "                'India Returns', \n",
    "                'China Returns',\n",
    "                'South Africa Returns'])\n",
    "\n",
    "    return df_predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions\n",
    "lstm_df = depict_results(lstm_predictions)\n",
    "xg_df = depict_results(xg_predictions)\n",
    "transformer_df = depict_results(transformer_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_mae = mean_absolute_error(lstm_df, y_test)\n",
    "xg_mae = mean_absolute_error(xg_df, y_test)\n",
    "transformer_mae = mean_absolute_error(transformer_df, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.011240764634263361, 0.019261630538247483, 0.15631945028397828)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_mae, xg_mae, transformer_mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_mse = mean_squared_error(lstm_df, y_test)\n",
    "xg_mse = mean_squared_error(xg_df, y_test)\n",
    "transformer_mse = mean_squared_error(transformer_df, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0005721479061473556, 0.0009379368377660016, 0.03145842950130927)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_mse, xg_mse, transformer_mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aa46f9d99ecb56874a909196cc8fe851fdad884172da2927a3a528c17ec588e0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
