{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import datetime as dt\n",
    "import torch as torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch.nn as nn \n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, MaxAbsScaler, RobustScaler\n",
    "\n",
    "from scipy.special import softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('clean_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Brazil Open</th>\n",
       "      <th>Brazil High</th>\n",
       "      <th>Brazil Low</th>\n",
       "      <th>Brazil Close</th>\n",
       "      <th>Brazil Adj Close</th>\n",
       "      <th>Brazil Volume</th>\n",
       "      <th>Brazil Returns</th>\n",
       "      <th>Russia Open</th>\n",
       "      <th>Russia High</th>\n",
       "      <th>Russia Low</th>\n",
       "      <th>Russia Close</th>\n",
       "      <th>Russia Adj Close</th>\n",
       "      <th>Russia Volume</th>\n",
       "      <th>Russia Returns</th>\n",
       "      <th>India Open</th>\n",
       "      <th>India High</th>\n",
       "      <th>India Low</th>\n",
       "      <th>India Close</th>\n",
       "      <th>India Adj Close</th>\n",
       "      <th>India Volume</th>\n",
       "      <th>India Returns</th>\n",
       "      <th>China Open</th>\n",
       "      <th>China High</th>\n",
       "      <th>China Low</th>\n",
       "      <th>China Close</th>\n",
       "      <th>China Adj Close</th>\n",
       "      <th>China Volume</th>\n",
       "      <th>China Returns</th>\n",
       "      <th>South Africa Open</th>\n",
       "      <th>South Africa High</th>\n",
       "      <th>South Africa Low</th>\n",
       "      <th>South Africa Close</th>\n",
       "      <th>South Africa Adj Close</th>\n",
       "      <th>South Africa Volume</th>\n",
       "      <th>South Africa Returns</th>\n",
       "      <th>year</th>\n",
       "      <th>sin_day</th>\n",
       "      <th>cos_day</th>\n",
       "      <th>sin_day_of_week</th>\n",
       "      <th>cos_day_of_week</th>\n",
       "      <th>sin_month</th>\n",
       "      <th>cos_month</th>\n",
       "      <th>sin_week_of_year</th>\n",
       "      <th>cos_week_of_year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>78.500000</td>\n",
       "      <td>79.370003</td>\n",
       "      <td>78.070000</td>\n",
       "      <td>79.220001</td>\n",
       "      <td>53.268791</td>\n",
       "      <td>1.606910e+07</td>\n",
       "      <td>0.022062</td>\n",
       "      <td>149.070007</td>\n",
       "      <td>150.550003</td>\n",
       "      <td>149.070007</td>\n",
       "      <td>150.550003</td>\n",
       "      <td>150.550003</td>\n",
       "      <td>1043.0</td>\n",
       "      <td>0.018675</td>\n",
       "      <td>10.477620</td>\n",
       "      <td>10.477620</td>\n",
       "      <td>10.477620</td>\n",
       "      <td>10.477620</td>\n",
       "      <td>10.477620</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000995</td>\n",
       "      <td>52.580002</td>\n",
       "      <td>52.720001</td>\n",
       "      <td>52.529999</td>\n",
       "      <td>52.619999</td>\n",
       "      <td>42.883629</td>\n",
       "      <td>2900.000000</td>\n",
       "      <td>0.013873</td>\n",
       "      <td>3412.7334</td>\n",
       "      <td>3412.733400</td>\n",
       "      <td>3412.7334</td>\n",
       "      <td>3457.8498</td>\n",
       "      <td>3457.8498</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>0.028368</td>\n",
       "      <td>2011</td>\n",
       "      <td>0.258819</td>\n",
       "      <td>0.965926</td>\n",
       "      <td>-0.433884</td>\n",
       "      <td>-0.900969</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.123234e-17</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-1.608123e-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>78.846667</td>\n",
       "      <td>79.520002</td>\n",
       "      <td>78.390000</td>\n",
       "      <td>79.406667</td>\n",
       "      <td>53.394311</td>\n",
       "      <td>1.444657e+07</td>\n",
       "      <td>0.002356</td>\n",
       "      <td>150.013336</td>\n",
       "      <td>151.363337</td>\n",
       "      <td>150.013336</td>\n",
       "      <td>151.206670</td>\n",
       "      <td>151.206670</td>\n",
       "      <td>2127.0</td>\n",
       "      <td>0.004362</td>\n",
       "      <td>10.483197</td>\n",
       "      <td>10.483197</td>\n",
       "      <td>10.483197</td>\n",
       "      <td>10.483197</td>\n",
       "      <td>10.483197</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000532</td>\n",
       "      <td>52.980001</td>\n",
       "      <td>53.073334</td>\n",
       "      <td>52.726666</td>\n",
       "      <td>52.993333</td>\n",
       "      <td>43.187885</td>\n",
       "      <td>5433.333333</td>\n",
       "      <td>0.007095</td>\n",
       "      <td>3430.2784</td>\n",
       "      <td>3434.580533</td>\n",
       "      <td>3430.2784</td>\n",
       "      <td>3463.5826</td>\n",
       "      <td>3463.5826</td>\n",
       "      <td>3599.666667</td>\n",
       "      <td>0.001658</td>\n",
       "      <td>2011</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>-0.974928</td>\n",
       "      <td>-0.222521</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.123234e-17</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-1.608123e-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>79.193334</td>\n",
       "      <td>79.670001</td>\n",
       "      <td>78.709999</td>\n",
       "      <td>79.593333</td>\n",
       "      <td>53.519831</td>\n",
       "      <td>1.282403e+07</td>\n",
       "      <td>0.002351</td>\n",
       "      <td>150.956665</td>\n",
       "      <td>152.176671</td>\n",
       "      <td>150.956665</td>\n",
       "      <td>151.863337</td>\n",
       "      <td>151.863337</td>\n",
       "      <td>3211.0</td>\n",
       "      <td>0.004343</td>\n",
       "      <td>10.488773</td>\n",
       "      <td>10.488773</td>\n",
       "      <td>10.488773</td>\n",
       "      <td>10.488773</td>\n",
       "      <td>10.488773</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000532</td>\n",
       "      <td>53.380000</td>\n",
       "      <td>53.426666</td>\n",
       "      <td>52.923332</td>\n",
       "      <td>53.366668</td>\n",
       "      <td>43.492142</td>\n",
       "      <td>7966.666667</td>\n",
       "      <td>0.007045</td>\n",
       "      <td>3447.8234</td>\n",
       "      <td>3456.427667</td>\n",
       "      <td>3447.8234</td>\n",
       "      <td>3469.3154</td>\n",
       "      <td>3469.3154</td>\n",
       "      <td>6199.333333</td>\n",
       "      <td>0.001655</td>\n",
       "      <td>2011</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>-0.781831</td>\n",
       "      <td>0.623490</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.123234e-17</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-1.608123e-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>79.540001</td>\n",
       "      <td>79.820000</td>\n",
       "      <td>79.029999</td>\n",
       "      <td>79.779999</td>\n",
       "      <td>53.645351</td>\n",
       "      <td>1.120150e+07</td>\n",
       "      <td>0.002345</td>\n",
       "      <td>151.899994</td>\n",
       "      <td>152.990005</td>\n",
       "      <td>151.899994</td>\n",
       "      <td>152.520004</td>\n",
       "      <td>152.520004</td>\n",
       "      <td>4295.0</td>\n",
       "      <td>0.004324</td>\n",
       "      <td>10.494350</td>\n",
       "      <td>10.494350</td>\n",
       "      <td>10.494350</td>\n",
       "      <td>10.494350</td>\n",
       "      <td>10.494350</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000532</td>\n",
       "      <td>53.779999</td>\n",
       "      <td>53.779999</td>\n",
       "      <td>53.119999</td>\n",
       "      <td>53.740002</td>\n",
       "      <td>43.796398</td>\n",
       "      <td>10500.000000</td>\n",
       "      <td>0.006996</td>\n",
       "      <td>3465.3684</td>\n",
       "      <td>3478.274800</td>\n",
       "      <td>3465.3684</td>\n",
       "      <td>3475.0482</td>\n",
       "      <td>3475.0482</td>\n",
       "      <td>8799.000000</td>\n",
       "      <td>0.001652</td>\n",
       "      <td>2011</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.123234e-17</td>\n",
       "      <td>0.992709</td>\n",
       "      <td>-1.205367e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>79.449997</td>\n",
       "      <td>80.080002</td>\n",
       "      <td>79.230003</td>\n",
       "      <td>79.510002</td>\n",
       "      <td>53.463795</td>\n",
       "      <td>1.189970e+07</td>\n",
       "      <td>-0.003384</td>\n",
       "      <td>151.160004</td>\n",
       "      <td>152.160004</td>\n",
       "      <td>151.160004</td>\n",
       "      <td>152.160004</td>\n",
       "      <td>152.160004</td>\n",
       "      <td>8470.0</td>\n",
       "      <td>-0.002360</td>\n",
       "      <td>10.456600</td>\n",
       "      <td>10.456600</td>\n",
       "      <td>10.456600</td>\n",
       "      <td>10.456600</td>\n",
       "      <td>10.456600</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.003597</td>\n",
       "      <td>53.500000</td>\n",
       "      <td>53.560001</td>\n",
       "      <td>53.400002</td>\n",
       "      <td>53.419998</td>\n",
       "      <td>43.535599</td>\n",
       "      <td>3700.000000</td>\n",
       "      <td>-0.005955</td>\n",
       "      <td>3477.7798</td>\n",
       "      <td>3477.779800</td>\n",
       "      <td>3477.7798</td>\n",
       "      <td>3477.7798</td>\n",
       "      <td>3477.7798</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000786</td>\n",
       "      <td>2011</td>\n",
       "      <td>0.965926</td>\n",
       "      <td>0.258819</td>\n",
       "      <td>0.781831</td>\n",
       "      <td>0.623490</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.123234e-17</td>\n",
       "      <td>0.992709</td>\n",
       "      <td>-1.205367e-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  Brazil Open  ...  sin_week_of_year  cos_week_of_year\n",
       "0           0    78.500000  ...          1.000000     -1.608123e-16\n",
       "1           1    78.846667  ...          1.000000     -1.608123e-16\n",
       "2           2    79.193334  ...          1.000000     -1.608123e-16\n",
       "3           3    79.540001  ...          0.992709     -1.205367e-01\n",
       "4           4    79.449997  ...          0.992709     -1.205367e-01\n",
       "\n",
       "[5 rows x 45 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns={'Unnamed: 0'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Brazil Open</th>\n",
       "      <th>Brazil High</th>\n",
       "      <th>Brazil Low</th>\n",
       "      <th>Brazil Close</th>\n",
       "      <th>Brazil Adj Close</th>\n",
       "      <th>Brazil Volume</th>\n",
       "      <th>Brazil Returns</th>\n",
       "      <th>Russia Open</th>\n",
       "      <th>Russia High</th>\n",
       "      <th>Russia Low</th>\n",
       "      <th>Russia Close</th>\n",
       "      <th>Russia Adj Close</th>\n",
       "      <th>Russia Volume</th>\n",
       "      <th>Russia Returns</th>\n",
       "      <th>India Open</th>\n",
       "      <th>India High</th>\n",
       "      <th>India Low</th>\n",
       "      <th>India Close</th>\n",
       "      <th>India Adj Close</th>\n",
       "      <th>India Volume</th>\n",
       "      <th>India Returns</th>\n",
       "      <th>China Open</th>\n",
       "      <th>China High</th>\n",
       "      <th>China Low</th>\n",
       "      <th>China Close</th>\n",
       "      <th>China Adj Close</th>\n",
       "      <th>China Volume</th>\n",
       "      <th>China Returns</th>\n",
       "      <th>South Africa Open</th>\n",
       "      <th>South Africa High</th>\n",
       "      <th>South Africa Low</th>\n",
       "      <th>South Africa Close</th>\n",
       "      <th>South Africa Adj Close</th>\n",
       "      <th>South Africa Volume</th>\n",
       "      <th>South Africa Returns</th>\n",
       "      <th>year</th>\n",
       "      <th>sin_day</th>\n",
       "      <th>cos_day</th>\n",
       "      <th>sin_day_of_week</th>\n",
       "      <th>cos_day_of_week</th>\n",
       "      <th>sin_month</th>\n",
       "      <th>cos_month</th>\n",
       "      <th>sin_week_of_year</th>\n",
       "      <th>cos_week_of_year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>78.500000</td>\n",
       "      <td>79.370003</td>\n",
       "      <td>78.070000</td>\n",
       "      <td>79.220001</td>\n",
       "      <td>53.268791</td>\n",
       "      <td>1.606910e+07</td>\n",
       "      <td>0.022062</td>\n",
       "      <td>149.070007</td>\n",
       "      <td>150.550003</td>\n",
       "      <td>149.070007</td>\n",
       "      <td>150.550003</td>\n",
       "      <td>150.550003</td>\n",
       "      <td>1043.0</td>\n",
       "      <td>0.018675</td>\n",
       "      <td>10.477620</td>\n",
       "      <td>10.477620</td>\n",
       "      <td>10.477620</td>\n",
       "      <td>10.477620</td>\n",
       "      <td>10.477620</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000995</td>\n",
       "      <td>52.580002</td>\n",
       "      <td>52.720001</td>\n",
       "      <td>52.529999</td>\n",
       "      <td>52.619999</td>\n",
       "      <td>42.883629</td>\n",
       "      <td>2900.000000</td>\n",
       "      <td>0.013873</td>\n",
       "      <td>3412.7334</td>\n",
       "      <td>3412.733400</td>\n",
       "      <td>3412.7334</td>\n",
       "      <td>3457.8498</td>\n",
       "      <td>3457.8498</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>0.028368</td>\n",
       "      <td>2011</td>\n",
       "      <td>0.258819</td>\n",
       "      <td>0.965926</td>\n",
       "      <td>-0.433884</td>\n",
       "      <td>-0.900969</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.123234e-17</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-1.608123e-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>78.846667</td>\n",
       "      <td>79.520002</td>\n",
       "      <td>78.390000</td>\n",
       "      <td>79.406667</td>\n",
       "      <td>53.394311</td>\n",
       "      <td>1.444657e+07</td>\n",
       "      <td>0.002356</td>\n",
       "      <td>150.013336</td>\n",
       "      <td>151.363337</td>\n",
       "      <td>150.013336</td>\n",
       "      <td>151.206670</td>\n",
       "      <td>151.206670</td>\n",
       "      <td>2127.0</td>\n",
       "      <td>0.004362</td>\n",
       "      <td>10.483197</td>\n",
       "      <td>10.483197</td>\n",
       "      <td>10.483197</td>\n",
       "      <td>10.483197</td>\n",
       "      <td>10.483197</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000532</td>\n",
       "      <td>52.980001</td>\n",
       "      <td>53.073334</td>\n",
       "      <td>52.726666</td>\n",
       "      <td>52.993333</td>\n",
       "      <td>43.187885</td>\n",
       "      <td>5433.333333</td>\n",
       "      <td>0.007095</td>\n",
       "      <td>3430.2784</td>\n",
       "      <td>3434.580533</td>\n",
       "      <td>3430.2784</td>\n",
       "      <td>3463.5826</td>\n",
       "      <td>3463.5826</td>\n",
       "      <td>3599.666667</td>\n",
       "      <td>0.001658</td>\n",
       "      <td>2011</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>-0.974928</td>\n",
       "      <td>-0.222521</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.123234e-17</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-1.608123e-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>79.193334</td>\n",
       "      <td>79.670001</td>\n",
       "      <td>78.709999</td>\n",
       "      <td>79.593333</td>\n",
       "      <td>53.519831</td>\n",
       "      <td>1.282403e+07</td>\n",
       "      <td>0.002351</td>\n",
       "      <td>150.956665</td>\n",
       "      <td>152.176671</td>\n",
       "      <td>150.956665</td>\n",
       "      <td>151.863337</td>\n",
       "      <td>151.863337</td>\n",
       "      <td>3211.0</td>\n",
       "      <td>0.004343</td>\n",
       "      <td>10.488773</td>\n",
       "      <td>10.488773</td>\n",
       "      <td>10.488773</td>\n",
       "      <td>10.488773</td>\n",
       "      <td>10.488773</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000532</td>\n",
       "      <td>53.380000</td>\n",
       "      <td>53.426666</td>\n",
       "      <td>52.923332</td>\n",
       "      <td>53.366668</td>\n",
       "      <td>43.492142</td>\n",
       "      <td>7966.666667</td>\n",
       "      <td>0.007045</td>\n",
       "      <td>3447.8234</td>\n",
       "      <td>3456.427667</td>\n",
       "      <td>3447.8234</td>\n",
       "      <td>3469.3154</td>\n",
       "      <td>3469.3154</td>\n",
       "      <td>6199.333333</td>\n",
       "      <td>0.001655</td>\n",
       "      <td>2011</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>-0.781831</td>\n",
       "      <td>0.623490</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.123234e-17</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-1.608123e-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>79.540001</td>\n",
       "      <td>79.820000</td>\n",
       "      <td>79.029999</td>\n",
       "      <td>79.779999</td>\n",
       "      <td>53.645351</td>\n",
       "      <td>1.120150e+07</td>\n",
       "      <td>0.002345</td>\n",
       "      <td>151.899994</td>\n",
       "      <td>152.990005</td>\n",
       "      <td>151.899994</td>\n",
       "      <td>152.520004</td>\n",
       "      <td>152.520004</td>\n",
       "      <td>4295.0</td>\n",
       "      <td>0.004324</td>\n",
       "      <td>10.494350</td>\n",
       "      <td>10.494350</td>\n",
       "      <td>10.494350</td>\n",
       "      <td>10.494350</td>\n",
       "      <td>10.494350</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000532</td>\n",
       "      <td>53.779999</td>\n",
       "      <td>53.779999</td>\n",
       "      <td>53.119999</td>\n",
       "      <td>53.740002</td>\n",
       "      <td>43.796398</td>\n",
       "      <td>10500.000000</td>\n",
       "      <td>0.006996</td>\n",
       "      <td>3465.3684</td>\n",
       "      <td>3478.274800</td>\n",
       "      <td>3465.3684</td>\n",
       "      <td>3475.0482</td>\n",
       "      <td>3475.0482</td>\n",
       "      <td>8799.000000</td>\n",
       "      <td>0.001652</td>\n",
       "      <td>2011</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.123234e-17</td>\n",
       "      <td>0.992709</td>\n",
       "      <td>-1.205367e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>79.449997</td>\n",
       "      <td>80.080002</td>\n",
       "      <td>79.230003</td>\n",
       "      <td>79.510002</td>\n",
       "      <td>53.463795</td>\n",
       "      <td>1.189970e+07</td>\n",
       "      <td>-0.003384</td>\n",
       "      <td>151.160004</td>\n",
       "      <td>152.160004</td>\n",
       "      <td>151.160004</td>\n",
       "      <td>152.160004</td>\n",
       "      <td>152.160004</td>\n",
       "      <td>8470.0</td>\n",
       "      <td>-0.002360</td>\n",
       "      <td>10.456600</td>\n",
       "      <td>10.456600</td>\n",
       "      <td>10.456600</td>\n",
       "      <td>10.456600</td>\n",
       "      <td>10.456600</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.003597</td>\n",
       "      <td>53.500000</td>\n",
       "      <td>53.560001</td>\n",
       "      <td>53.400002</td>\n",
       "      <td>53.419998</td>\n",
       "      <td>43.535599</td>\n",
       "      <td>3700.000000</td>\n",
       "      <td>-0.005955</td>\n",
       "      <td>3477.7798</td>\n",
       "      <td>3477.779800</td>\n",
       "      <td>3477.7798</td>\n",
       "      <td>3477.7798</td>\n",
       "      <td>3477.7798</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000786</td>\n",
       "      <td>2011</td>\n",
       "      <td>0.965926</td>\n",
       "      <td>0.258819</td>\n",
       "      <td>0.781831</td>\n",
       "      <td>0.623490</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.123234e-17</td>\n",
       "      <td>0.992709</td>\n",
       "      <td>-1.205367e-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Brazil Open  Brazil High  ...  sin_week_of_year  cos_week_of_year\n",
       "0    78.500000    79.370003  ...          1.000000     -1.608123e-16\n",
       "1    78.846667    79.520002  ...          1.000000     -1.608123e-16\n",
       "2    79.193334    79.670001  ...          1.000000     -1.608123e-16\n",
       "3    79.540001    79.820000  ...          0.992709     -1.205367e-01\n",
       "4    79.449997    80.080002  ...          0.992709     -1.205367e-01\n",
       "\n",
       "[5 rows x 44 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_label_split(df, target_cols):\n",
    "    targets = [target for target in target_cols]\n",
    "    y = df[targets]\n",
    "    X = df.drop(columns=targets)\n",
    "    return X, y\n",
    "\n",
    "def train_val_test_split(df, target_cols, test_ratio):\n",
    "    val_ratio = test_ratio / (1 - test_ratio)\n",
    "    X, y = feature_label_split(df, target_cols)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_ratio, shuffle=False)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=val_ratio, shuffle=False)\n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test\n",
    "\n",
    "returns = [returns for returns in df.columns if 'Returns' in returns] # unreal stuff, don't ask me how \n",
    "X_train, X_val, X_test, y_train, y_val, y_test = train_val_test_split(df, returns, 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Brazil Returns</th>\n",
       "      <th>Russia Returns</th>\n",
       "      <th>India Returns</th>\n",
       "      <th>China Returns</th>\n",
       "      <th>South Africa Returns</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.022062</td>\n",
       "      <td>0.018675</td>\n",
       "      <td>0.000995</td>\n",
       "      <td>0.013873</td>\n",
       "      <td>0.028368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.002356</td>\n",
       "      <td>0.004362</td>\n",
       "      <td>0.000532</td>\n",
       "      <td>0.007095</td>\n",
       "      <td>0.001658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.002351</td>\n",
       "      <td>0.004343</td>\n",
       "      <td>0.000532</td>\n",
       "      <td>0.007045</td>\n",
       "      <td>0.001655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.002345</td>\n",
       "      <td>0.004324</td>\n",
       "      <td>0.000532</td>\n",
       "      <td>0.006996</td>\n",
       "      <td>0.001652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.003384</td>\n",
       "      <td>-0.002360</td>\n",
       "      <td>-0.003597</td>\n",
       "      <td>-0.005955</td>\n",
       "      <td>0.000786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2404</th>\n",
       "      <td>-0.011406</td>\n",
       "      <td>0.004094</td>\n",
       "      <td>0.004940</td>\n",
       "      <td>-0.002571</td>\n",
       "      <td>0.002153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2405</th>\n",
       "      <td>0.000249</td>\n",
       "      <td>-0.012957</td>\n",
       "      <td>-0.003214</td>\n",
       "      <td>0.009742</td>\n",
       "      <td>0.004382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2406</th>\n",
       "      <td>-0.006721</td>\n",
       "      <td>0.012812</td>\n",
       "      <td>-0.020878</td>\n",
       "      <td>0.006891</td>\n",
       "      <td>0.003056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2407</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.006118</td>\n",
       "      <td>-0.009533</td>\n",
       "      <td>0.001521</td>\n",
       "      <td>0.006476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2408</th>\n",
       "      <td>-0.014787</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.016247</td>\n",
       "      <td>-0.000304</td>\n",
       "      <td>-0.008408</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2409 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Brazil Returns  Russia Returns  India Returns  China Returns  \\\n",
       "0           0.022062        0.018675       0.000995       0.013873   \n",
       "1           0.002356        0.004362       0.000532       0.007095   \n",
       "2           0.002351        0.004343       0.000532       0.007045   \n",
       "3           0.002345        0.004324       0.000532       0.006996   \n",
       "4          -0.003384       -0.002360      -0.003597      -0.005955   \n",
       "...              ...             ...            ...            ...   \n",
       "2404       -0.011406        0.004094       0.004940      -0.002571   \n",
       "2405        0.000249       -0.012957      -0.003214       0.009742   \n",
       "2406       -0.006721        0.012812      -0.020878       0.006891   \n",
       "2407        0.000000       -0.006118      -0.009533       0.001521   \n",
       "2408       -0.014787        0.000000       0.016247      -0.000304   \n",
       "\n",
       "      South Africa Returns  \n",
       "0                 0.028368  \n",
       "1                 0.001658  \n",
       "2                 0.001655  \n",
       "3                 0.001652  \n",
       "4                 0.000786  \n",
       "...                    ...  \n",
       "2404              0.002153  \n",
       "2405              0.004382  \n",
       "2406              0.003056  \n",
       "2407              0.006476  \n",
       "2408             -0.008408  \n",
       "\n",
       "[2409 rows x 5 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "\n",
    "X_train_arr = scaler.fit_transform(X_train)\n",
    "X_val_arr = scaler.transform(X_val)\n",
    "X_test_arr = scaler.transform(X_test)\n",
    "\n",
    "y_train_arr = scaler.fit_transform(y_train)\n",
    "y_val_arr = scaler.transform(y_val)\n",
    "y_test_arr = scaler.transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "train_features = torch.Tensor(X_train_arr)\n",
    "train_targets = torch.Tensor(y_train_arr)\n",
    "val_features = torch.Tensor(X_val_arr)\n",
    "val_targets = torch.Tensor(y_val_arr)\n",
    "test_features = torch.Tensor(X_test_arr)\n",
    "test_targets = torch.Tensor(y_test_arr)\n",
    "\n",
    "train = TensorDataset(train_features, train_targets)\n",
    "val = TensorDataset(val_features, val_targets)\n",
    "test = TensorDataset(test_features, test_targets)\n",
    "\n",
    "train_loader = DataLoader(train, batch_size=batch_size, shuffle=False, drop_last=True)\n",
    "val_loader = DataLoader(val, batch_size=batch_size, shuffle=False, drop_last=True)\n",
    "test_loader = DataLoader(test, batch_size=batch_size, shuffle=False, drop_last=True)\n",
    "test_loader_one = DataLoader(test, batch_size=1, shuffle=False, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.7362, 0.4679, 0.4970, 0.6123, 0.6899],\n",
       "        [0.6579, 0.4117, 0.4943, 0.5690, 0.5185],\n",
       "        [0.6579, 0.4116, 0.4943, 0.5687, 0.5185],\n",
       "        ...,\n",
       "        [0.6218, 0.4448, 0.3708, 0.5677, 0.5275],\n",
       "        [0.6485, 0.3705, 0.4362, 0.5335, 0.5494],\n",
       "        [0.5898, 0.3945, 0.5851, 0.5218, 0.4539]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cpu', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, layer_dim, output_dim, dropout_prob):\n",
    "        super(LSTMModel, self).__init__()\n",
    "\n",
    "        # Defining the number of layers and the nodes in each layer\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.layer_dim = layer_dim\n",
    "\n",
    "        # LSTM layers\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_dim, hidden_dim, layer_dim, batch_first=True, dropout=dropout_prob\n",
    "        )\n",
    "\n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Initializing hidden state for first input with zeros\n",
    "        h0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).requires_grad_()\n",
    "\n",
    "        # Initializing cell state for first input with zeros\n",
    "        c0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).requires_grad_()\n",
    "\n",
    "        # We need to detach as we are doing truncated backpropagation through time (BPTT)\n",
    "        # If we don't, we'll backprop all the way to the start even after going through another batch\n",
    "        # Forward propagation by passing in the input, hidden state, and cell state into the model\n",
    "        out, (hn, cn) = self.lstm(x, (h0.detach(), c0.detach()))\n",
    "\n",
    "        # Reshaping the outputs in the shape of (batch_size, seq_length, hidden_size)\n",
    "        # so that it can fit into the fully connected layer\n",
    "        out = out[:, -1, :]\n",
    "\n",
    "        # Convert the final state to our desired output shape (batch_size, output_dim)\n",
    "        out = self.fc(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(model, model_params):\n",
    "    models = {\n",
    "        \"lstm\": LSTMModel,\n",
    "    }\n",
    "    return models.get(model.lower())(**model_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimization:\n",
    "    def __init__(self, model, loss_fn, optimizer):\n",
    "        self.model = model\n",
    "        self.loss_fn = loss_fn\n",
    "        self.optimizer = optimizer\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "    \n",
    "    def train_step(self, x, y):\n",
    "        # Sets model to train mode\n",
    "        self.model.train()\n",
    "\n",
    "        # Makes predictions\n",
    "        yhat = self.model(x)\n",
    "\n",
    "        # Computes loss\n",
    "        loss = self.loss_fn(y, yhat)\n",
    "\n",
    "        # Computes gradients\n",
    "        loss.backward()\n",
    "\n",
    "        # Updates parameters and zeroes gradients\n",
    "        self.optimizer.step()\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "        # Returns the loss\n",
    "        return loss.item()\n",
    "\n",
    "    def train(self, train_loader, val_loader, batch_size=64, n_epochs=50, n_features=1):\n",
    "        # model_path = f'models/{self.model}_{dt.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")}.pt'\n",
    "        model_path = 'models/model.pt'\n",
    "        \n",
    "        for epoch in range(1, n_epochs + 1):\n",
    "            batch_losses = []\n",
    "            for x_batch, y_batch in train_loader:\n",
    "                x_batch = x_batch.view([batch_size, -1, n_features]).to(device)\n",
    "                y_batch = y_batch.to(device)\n",
    "                loss = self.train_step(x_batch, y_batch)\n",
    "                batch_losses.append(loss)\n",
    "            training_loss = np.mean(batch_losses)\n",
    "            self.train_losses.append(training_loss)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                batch_val_losses = []\n",
    "                for x_val, y_val in val_loader:\n",
    "                    x_val = x_val.view([batch_size, -1, n_features]).to(device)\n",
    "                    y_val = y_val.to(device)\n",
    "                    self.model.eval()\n",
    "                    yhat = self.model(x_val)\n",
    "                    val_loss = self.loss_fn(y_val, yhat).item()\n",
    "                    batch_val_losses.append(val_loss)\n",
    "                validation_loss = np.mean(batch_val_losses)\n",
    "                self.val_losses.append(validation_loss)\n",
    "\n",
    "            if (epoch <= 10) | (epoch % 50 == 0):\n",
    "                print(\n",
    "                    f\"[{epoch}/{n_epochs}] Training loss: {training_loss:.4f}\\t Validation loss: {validation_loss:.4f}\"\n",
    "                )\n",
    "\n",
    "        torch.save(self.model.state_dict(), model_path)\n",
    "\n",
    "    def evaluate(self, test_loader, batch_size=1, n_features=1):\n",
    "\n",
    "        with torch.no_grad():\n",
    "            predictions = []\n",
    "            values = []\n",
    "            for x_test, y_test in test_loader:\n",
    "                x_test = x_test.view([batch_size, -1, n_features]).to(device)\n",
    "                y_test = y_test.to(device)\n",
    "                self.model.eval()\n",
    "                yhat = self.model(x_test)\n",
    "                predictions.append(yhat.to(device).detach().numpy())\n",
    "                values.append(y_test.to(device).detach().numpy())\n",
    "\n",
    "        return predictions, values\n",
    "\n",
    "    def plot_losses(self):\n",
    "        plt.plot(self.train_losses, label=\"Training loss\")\n",
    "        plt.plot(self.val_losses, label=\"Validation loss\")\n",
    "        plt.legend()\n",
    "        plt.title(\"Losses\")\n",
    "        plt.show()\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/100] Training loss: 0.1471\t Validation loss: 0.0644\n",
      "[2/100] Training loss: 0.0095\t Validation loss: 0.0174\n",
      "[3/100] Training loss: 0.0074\t Validation loss: 0.0121\n",
      "[4/100] Training loss: 0.0067\t Validation loss: 0.0097\n",
      "[5/100] Training loss: 0.0063\t Validation loss: 0.0083\n",
      "[6/100] Training loss: 0.0061\t Validation loss: 0.0071\n",
      "[7/100] Training loss: 0.0059\t Validation loss: 0.0063\n",
      "[8/100] Training loss: 0.0057\t Validation loss: 0.0060\n",
      "[9/100] Training loss: 0.0056\t Validation loss: 0.0051\n",
      "[10/100] Training loss: 0.0054\t Validation loss: 0.0049\n",
      "[50/100] Training loss: 0.0047\t Validation loss: 0.0045\n",
      "[100/100] Training loss: 0.0047\t Validation loss: 0.0045\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAEICAYAAABRSj9aAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAnPElEQVR4nO3de3hU9b3v8fd3rckFEvACUZFYwV2UosilkbLFUrz0VNSKtfapPFa89Ghttd5aW+xN2m732eeUp9vNOSoP9dJa3WX3aOuhltZdvBTdvRHQB0VEo2JNQY1YIAoJmZnv+WOtGSbJhExCQmDl83qeeZhZa/3W+v0mwye//NZavzF3R0REkisY6AqIiEj/UtCLiCScgl5EJOEU9CIiCaegFxFJOAW9iEjCKehFRBJOQS+JZ2YbzeyMga6HyEBR0IuIJJyCXgYlM6sws9vMbFP8uM3MKuJ1I83sETPbambvmtlTZhbE675uZn8zs2Yz22Bmp8fLAzObb2avmNkWM/u5mR0ar6s0s/vj5VvNbJWZHT5wrZfBRkEvg9U3genAZGASMA34VrzuK0AjUAMcDnwDcDM7DrgGOMndhwGfADbGZa4FzgM+BhwJ/B24PV53CXAQcBQwArgK2NlfDRPpSEEvg9VFwPfc/W13bwK+C1wcr2sDRgFHu3ubuz/l0aRQGaACmGBmZe6+0d1fict8Afimuze6eyuwALjAzFLx/kYAH3T3jLuvdvft+6ylMugp6GWwOhJ4veD16/EygB8ADcB/mtmrZjYfwN0bgOuJQvxtM1tqZrkyRwO/jIdmtgLriX4xHA78FHgUWBoPE/0vMyvrz8aJFFLQy2C1iSiccz4QL8Pdm939K+5+DPBJ4MbcWLy7/7u7nxKXdeB/xuXfAGa7+8EFj0p3/1v8V8F33X0CcDJwDjBvn7RSBAW9DB5l8UnRSjOrBH4GfMvMasxsJPAd4H4AMzvHzD5oZgZsJ+qZZ8zsODM7LT5p20I0zp6J978YuNXMjo73UWNmc+Lnp5rZRDML4/21FZQT6XcKehkslhMFc+5RCdQDa4HngDXAP8XbjgNWAO8BfwTucPcnicbn/wV4B3gTOIzoRC3AvwHLiIZ7moE/AR+J1x0BPEgU8uuB3xP/UhHZF0xfPCIikmzq0YuIJJyCXkQk4RT0IiIJp6AXEUm41EBXoJiRI0f6mDFjBroaIiIHjNWrV7/j7jXF1u2XQT9mzBjq6+sHuhoiIgcMM3u9q3UauhERSTgFvYhIwinoRUQSbr8coxeRfautrY3GxkZaWloGuirSjcrKSmpraykrK30CVAW9iNDY2MiwYcMYM2YM0Vxusj9yd7Zs2UJjYyNjx44tuZyGbkSElpYWRowYoZDfz5kZI0aM6PFfXgp6EQFQyB8gevNzSlTQL3rsZX7/UtNAV0NEZL+SqKBf/PtXePplBb3IgWbLli1MnjyZyZMnc8QRRzB69Oj86127du2xbH19Pddee223xzj55JP7pK5PPvkk55xzTp/sa19J1MnYMDDSWc2vL3KgGTFiBM8++ywACxYsoLq6mq9+9av59el0mlSqeFzV1dVRV1fX7TH+8Ic/9EldD0SJ6tGnAiOjoBdJhEsvvZQbb7yRU089la9//ev85S9/4eSTT2bKlCmcfPLJbNiwAWjfw16wYAGXX345s2bN4phjjmHRokX5/VVXV+e3nzVrFhdccAHjx4/noosuIvcFTMuXL2f8+PGccsopXHvttd323N99913OO+88TjzxRKZPn87atWsB+P3vf5//i2TKlCk0NzezefNmZs6cyeTJkznhhBN46qmn+vw960rCevSBevQie+m7v1rHC5u29+k+Jxw5nFs+eXyPy7300kusWLGCMAzZvn07K1euJJVKsWLFCr7xjW/w0EMPdSrz4osv8sQTT9Dc3Mxxxx3HF7/4xU7XnD/zzDOsW7eOI488khkzZvBf//Vf1NXV8YUvfIGVK1cyduxY5s6d2239brnlFqZMmcLDDz/M448/zrx583j22WdZuHAht99+OzNmzOC9996jsrKSJUuW8IlPfIJvfvObZDIZduzY0eP3o7cSFfSpwMhkFPQiSfGZz3yGMAwB2LZtG5dccgkvv/wyZkZbW1vRMmeffTYVFRVUVFRw2GGH8dZbb1FbW9tum2nTpuWXTZ48mY0bN1JdXc0xxxyTvz597ty5LFmyZI/1e/rpp/O/bE477TS2bNnCtm3bmDFjBjfeeCMXXXQR559/PrW1tZx00klcfvnltLW1cd555zF58uS9eWt6JFFBrzF6kb3Xm553f6mqqso///a3v82pp57KL3/5SzZu3MisWbOKlqmoqMg/D8OQdDpd0ja9+f7sYmXMjPnz53P22WezfPlypk+fzooVK5g5cyYrV67k17/+NRdffDE33XQT8+bN6/ExeyNZY/Shkc5mB7oaItIPtm3bxujRowH48Y9/3Of7Hz9+PK+++iobN24E4D/+4z+6LTNz5kweeOABIBr7HzlyJMOHD+eVV15h4sSJfP3rX6euro4XX3yR119/ncMOO4wrrriCz3/+86xZs6bP29CVkoLezM40sw1m1mBm84usH29mfzSzVjP7apH1oZk9Y2aP9EWlu6IevUhyfe1rX+Pmm29mxowZZDKZPt//kCFDuOOOOzjzzDM55ZRTOPzwwznooIP2WGbBggXU19dz4oknMn/+fH7yk58AcNttt3HCCScwadIkhgwZwuzZs3nyySfzJ2cfeughrrvuuj5vQ1esuz9XzCwEXgI+DjQCq4C57v5CwTaHAUcD5wF/d/eFHfZxI1AHDHf3bi9Araur89588cgn/nUlY0dWsfjiD/e4rMhgtn79ej70oQ8NdDUG3HvvvUd1dTXuztVXX824ceO44YYbBrpanRT7eZnZancvep1pKT36aUCDu7/q7ruApcCcwg3c/W13XwV0OjtiZrXA2cBdpTWh99SjF5G98aMf/YjJkydz/PHHs23bNr7whS8MdJX6RCknY0cDbxS8bgQ+0oNj3AZ8DRi2p43M7ErgSoAPfOADPdj9bqnQyGiMXkR66YYbbtgve/B7q5QefbEZdErqNpvZOcDb7r66u23dfYm717l7XU1N0e+37ZZ69CIinZUS9I3AUQWva4FNJe5/BnCumW0kGvI5zczu71ENe0B3xoqIdFZK0K8CxpnZWDMrBy4ElpWyc3e/2d1r3X1MXO5xd/9cr2vbDfXoRUQ663aM3t3TZnYN8CgQAve4+zozuypev9jMjgDqgeFA1syuBya4e9/eR92NVBCwY1fnmyNERAazkq6jd/fl7n6su/+Du98aL1vs7ovj52/GPffh7n5w/Hx7h308WcqllXsj1NCNyAFp1qxZPProo+2W3XbbbXzpS1/aY5ncZdhnnXUWW7du7bTNggULWLhwYaflhR5++GFeeCF/tTjf+c53WLFiRQ9qX9z+NJ1xou6MLQs1dCNyIJo7dy5Lly5tt2zp0qUlTSwG0ayTBx98cK+O3THov/e973HGGWf0al/7q0QFvXr0IgemCy64gEceeYTW1lYANm7cyKZNmzjllFP44he/SF1dHccffzy33HJL0fJjxozhnXfeAeDWW2/luOOO44wzzshPZQzRNfInnXQSkyZN4tOf/jQ7duzgD3/4A8uWLeOmm25i8uTJvPLKK1x66aU8+OCDADz22GNMmTKFiRMncvnll+frN2bMGG655RamTp3KxIkTefHFF/fYvoGezjhRk5qlNE2xyN77zXx487m+3ecRE2H2v3S5esSIEUybNo3f/va3zJkzh6VLl/LZz34WM+PWW2/l0EMPJZPJcPrpp7N27VpOPPHEovtZvXo1S5cu5ZlnniGdTjN16lQ+/OHoTvnzzz+fK664AoBvfetb3H333Xz5y1/m3HPP5ZxzzuGCCy5ot6+WlhYuvfRSHnvsMY499ljmzZvHnXfeyfXXXw/AyJEjWbNmDXfccQcLFy7krru6vid0oKczVo9eRPYLhcM3hcM2P//5z5k6dSpTpkxh3bp17YZZOnrqqaf41Kc+xdChQxk+fDjnnntuft3zzz/PRz/6USZOnMgDDzzAunXr9lifDRs2MHbsWI499lgALrnkElauXJlff/755wPw4Q9/OD8RWleefvppLr74YqD4dMaLFi1i69atpFIpTjrpJO69914WLFjAc889x7Bhe7zXtCQJ69Fr9kqRvbaHnnd/Ou+887jxxhtZs2YNO3fuZOrUqbz22mssXLiQVatWccghh3DppZfS0tKyx/2YFbvHM/rGqocffphJkybx4x//mCeffHKP++luHrDcVMddTYXc3b725XTGyevR64tHRA5I1dXVzJo1i8svvzzfm9++fTtVVVUcdNBBvPXWW/zmN7/Z4z5mzpzJL3/5S3bu3ElzczO/+tWv8uuam5sZNWoUbW1t+amFAYYNG0Zzc3OnfY0fP56NGzfS0NAAwE9/+lM+9rGP9aptAz2dcbJ69KHRpqEbkQPW3LlzOf/88/NDOJMmTWLKlCkcf/zxHHPMMcyYMWOP5adOncpnP/tZJk+ezNFHH81HP/rR/Lrvf//7fOQjH+Hoo49m4sSJ+XC/8MILueKKK1i0aFH+JCxAZWUl9957L5/5zGdIp9OcdNJJXHXVVb1q14IFC7jssss48cQTGTp0aLvpjJ944gnCMGTChAnMnj2bpUuX8oMf/ICysjKqq6u57777enXMQt1OUzwQejtN8bcefo7lz73Jmm9/vB9qJZJcmqb4wNIf0xQfMFJBQDqjMXoRkUIJC3pddSMi0lGigj7UnbEivbY/DuNKZ735OSUq6NWjF+mdyspKtmzZorDfz7k7W7ZsobKyskflEnXVTRjfGevuXV5LKyKd1dbW0tjYSFNT00BXRbpRWVlJbW1tj8okKuhTQRTuWYdQOS9SsrKyMsaOHTvQ1ZB+kqihmzAO+jZdeSMikpeooM/16DVOLyKyW7KCPoyaoytvRER2S1bQq0cvItJJooI+N0avGSxFRHYrKejN7Ewz22BmDWY2v8j68Wb2RzNrNbOvFiw/ysyeMLP1ZrbOzK7ry8p3pB69iEhn3V5eaWYhcDvwcaARWGVmy9y9cPb/d4FrgfM6FE8DX3H3NWY2DFhtZr/rULbP5Hv0mqpYRCSvlB79NKDB3V91913AUmBO4Qbu/ra7rwLaOizf7O5r4ufNwHpgdJ/UvIhUmBu6UdCLiOSUEvSjgTcKXjfSi7A2szHAFODPXay/0szqzay+t3fnhUHUnIzG6EVE8koJ+mL3mPaoy2xm1cBDwPXuvr3YNu6+xN3r3L2upqamJ7vPSwXq0YuIdFRK0DcCRxW8rgU2lXoAMysjCvkH3P0XPatez6Q0Ri8i0kkpQb8KGGdmY82sHLgQWFbKzi2aWexuYL27/7D31SxNboxeV92IiOzW7VU37p42s2uAR4EQuMfd15nZVfH6xWZ2BFAPDAeyZnY9MAE4EbgYeM7Mno13+Q13X97nLWH3GL2GbkREditp9so4mJd3WLa44PmbREM6HT1N8TH+fqHr6EVEOtOdsSIiCZeooNfJWBGRzhIV9KGGbkREOklU0Kd0MlZEpJNkBX3+8kqN0YuI5CQr6HVnrIhIJ4kKeo3Ri4h0lqigz4/R66obEZG8RAV9qCkQREQ6SVTQ58bo23QyVkQkL1FBrzF6EZHOEhX0ujNWRKSzZAV9mPuGKQW9iEhOsoJe19GLiHSSqKDfPUavk7EiIjnJCnpTj15EpKNEBX0QGIFpjF5EpFCigh6iu2PbdNWNiEheSUFvZmea2QYzazCz+UXWjzezP5pZq5l9tSdl+1oYmMboRUQKdBv0ZhYCtwOzib7we66ZTeiw2bvAtcDCXpTtU6nQNEYvIlKglB79NKDB3V91913AUmBO4Qbu/ra7rwLaelq2r6UC0xi9iEiBUoJ+NPBGwevGeFkp9qZsr4RBoB69iEiBUoLeiiwrNUlLLmtmV5pZvZnVNzU1lbj7zlKBkdHJWBGRvFKCvhE4quB1LbCpxP2XXNbdl7h7nbvX1dTUlLj7zsJAY/QiIoVKCfpVwDgzG2tm5cCFwLIS9783ZXslFeqqGxGRQqnuNnD3tJldAzwKhMA97r7OzK6K1y82syOAemA4kDWz64EJ7r69WNl+agsQ9ejb1KMXEcnrNugB3H05sLzDssUFz98kGpYpqWx/0hi9iEh7ibwzVmP0IiK7JS/oNUYvItJO4oJeV92IiLSXuKDXnbEiIu0lLujVoxcRaS9xQZ8KAvXoRUQKJC7ow8BIZ3QyVkQkJ3FBn9LQjYhIO8kL+lAnY0VECiUv6HXDlIhIO4kL+lCXV4qItJO4oI/G6HUyVkQkJ3FBH2pSMxGRdhIX9KlQ0xSLiBRKXNBrjF5EpL3EBX0qCHTDlIhIgQQGvXr0IiKFEhf0Yag7Y0VECiUu6NWjFxFpr6SgN7MzzWyDmTWY2fwi683MFsXr15rZ1IJ1N5jZOjN73sx+ZmaVfdmAjsL4zlh3hb2ICJQQ9GYWArcDs4EJwFwzm9Bhs9nAuPhxJXBnXHY0cC1Q5+4nACFwYZ/VvohUYACoUy8iEimlRz8NaHD3V919F7AUmNNhmznAfR75E3CwmY2K16WAIWaWAoYCm/qo7kWFcdC36cobERGgtKAfDbxR8LoxXtbtNu7+N2Ah8FdgM7DN3f+z2EHM7Eozqzez+qamplLr30muR69xehGRSClBb0WWdUzRotuY2SFEvf2xwJFAlZl9rthB3H2Ju9e5e11NTU0J1Sou16PXlTciIpFSgr4ROKrgdS2dh1+62uYM4DV3b3L3NuAXwMm9r273ysKoSerRi4hESgn6VcA4MxtrZuVEJ1OXddhmGTAvvvpmOtEQzWaiIZvpZjbUzAw4HVjfh/XvZHePXmP0IiIQnSjdI3dPm9k1wKNEV83c4+7rzOyqeP1iYDlwFtAA7AAui9f92cweBNYAaeAZYEl/NCRHY/QiIu11G/QA7r6cKMwLly0ueO7A1V2UvQW4ZS/q2CP5Hr2mKhYRAZJ4Z2yoHr2ISKHEBX0YRE3SGL2ISCRxQZ/S5ZUiIu0kLug1Ri8i0l7igr5MY/QiIu0kLuh3j9Er6EVEIIFBr+voRUTaS1zQ685YEZH2Ehf06tGLiLSXuKDXVTciIu0lLuhTOhkrItJO4oI+zA/daIxeRAQSGPS56+jVoxcRiSQu6EOdjBURaSdxQZ8ynYwVESmUrKD/H0dxyJ/+GVCPXkQkJ1lBH4SEbTsAjdGLiOQkK+jLqwnS7wO6M1ZEJCdhQV9F0BYHvcboRUSAEoPezM40sw1m1mBm84usNzNbFK9fa2ZTC9YdbGYPmtmLZrbezP6xLxvQTnkVQTx0ozF6EZFIt0FvZiFwOzAbmADMNbMJHTabDYyLH1cCdxas+zfgt+4+HpgErO+DehdX2KNX0IuIAKX16KcBDe7+qrvvApYCczpsMwe4zyN/Ag42s1FmNhyYCdwN4O673H1r31W/g/JqLA563RkrIhIpJehHA28UvG6Ml5WyzTFAE3CvmT1jZneZWVWxg5jZlWZWb2b1TU1NJTegnfIq2KUevYhIoVKC3oos65iiXW2TAqYCd7r7FOB9oNMYP4C7L3H3Onevq6mpKaFaRZRXYbveJzCN0YuI5JQS9I3AUQWva4FNJW7TCDS6+5/j5Q8SBX//KK+GXe+TCgLadNWNiAhQWtCvAsaZ2VgzKwcuBJZ12GYZMC+++mY6sM3dN7v7m8AbZnZcvN3pwAt9VflOyquh7X1SgWuMXkQklupuA3dPm9k1wKNACNzj7uvM7Kp4/WJgOXAW0ADsAC4r2MWXgQfiXxKvdljXt8qj4f/qoE1j9CIisW6DHsDdlxOFeeGyxQXPHbi6i7LPAnW9r2IPxEE/PGjRGL2ISCxhd8ZWAzAsaFWPXkQklrCgj3r0w6yVjE7GiogACQ36avXoRUTyEhb00dBNtbXoqhsRkVjCgj7q0VdZK23q0YuIAIkN+haN0YuIxBIW9NHQTRUaoxcRyUlY0Ec9+qEaoxcRyUtW0KcqwEKqaFGPXkQklqygN4PyaoaiO2NFRHKSFfQA5VUMYad69CIiseQFfUU1Q7xVPXoRkVjygr68ikrfSTqjk7EiIpDIoK9miOtkrIhITgKDPurRa+hGRCSS2KBXj15EJJLMoM+qRy8ikpPAoK+mwneS1p2xIiJAiUFvZmea2QYzazCz+UXWm5ktitevNbOpHdaHZvaMmT3SVxXvUnkV5dmdZNIKehERKCHozSwEbgdmAxOAuWY2ocNms4Fx8eNK4M4O668D1u91bUtRXkWAE2Zb98nhRET2d6X06KcBDe7+qrvvApYCczpsMwe4zyN/Ag42s1EAZlYLnA3c1Yf17lo8g2V5duc+OZyIyP6ulKAfDbxR8LoxXlbqNrcBXwP2zVhKPINleXbHPjmciMj+rpSgtyLLOl7SUnQbMzsHeNvdV3d7ELMrzazezOqbmppKqFYX4qCvVI9eRAQoLegbgaMKXtcCm0rcZgZwrpltJBryOc3M7i92EHdf4u517l5XU1NTYvWLiIO+QkEvIgKUFvSrgHFmNtbMyoELgWUdtlkGzIuvvpkObHP3ze5+s7vXuvuYuNzj7v65vmxAJ/EYfaW39OthREQOFKnuNnD3tJldAzwKhMA97r7OzK6K1y8GlgNnAQ3ADuCy/qtyN3I9et+Ju2NWbFRJRGTw6DboAdx9OVGYFy5bXPDcgau72ceTwJM9rmFP5b83toWsQ6icF5FBLpF3xgIMtVbdHSsiQiKDPhq6qaKFdEbz3YiIJC/oy4bgGENNc9KLiEASg96MtnAoVfqCcBERIIlBD6RTUdBrjF5EJKlBHw6lytSjFxGBhAZ9JjWUoToZKyICJDXoy4ZSRat69CIiJDXoU0N11Y2ISCyRQZ9NVelkrIhILJFBnymLe/QaoxcRSWbQe1mVrqMXEYklNuiH0ko6o6EbEZFEBn22vJoyy5BN6wvCRUQSGfReFk1s5rveG+CaiIgMvEQGfW4GS1rfH9h6iIjsBxId9L5LQS8iktCgj758BAW9iEgyg97iHr1pjF5EpLSgN7MzzWyDmTWY2fwi683MFsXr15rZ1Hj5UWb2hJmtN7N1ZnZdXzegaH0roh69tSnoRUS6DXozC4HbgdnABGCumU3osNlsYFz8uBK4M16eBr7i7h8CpgNXFynb56wi6tEHbRq6EREppUc/DWhw91fdfRewFJjTYZs5wH0e+RNwsJmNcvfN7r4GwN2bgfXA6D6sf1FBxTAAbNeO/j6UiMh+r5SgHw28UfC6kc5h3e02ZjYGmAL8udhBzOxKM6s3s/qmpqYSqtW1oDIaugnVoxcRKSnorciyjpPI7HEbM6sGHgKud/ftxQ7i7kvcvc7d62pqakqoVtfC8qEABGn16EVESgn6RuCogte1wKZStzGzMqKQf8Ddf9H7qpYuTKXY4RUEafXoRURKCfpVwDgzG2tm5cCFwLIO2ywD5sVX30wHtrn7ZjMz4G5gvbv/sE9rvgepwHifClK7mvfVIUVE9lvdBr27p4FrgEeJTqb+3N3XmdlVZnZVvNly4FWgAfgR8KV4+QzgYuA0M3s2fpzV143oKAyMF7JjGL3lD5BJ9/fhRET2a6lSNnL35URhXrhsccFzB64uUu5pio/f96tUEHB/5gw+1vpDeOk38KFP7usqiIjsNxJ5Z2wqNB7PTqG54nD4y48GujoiIgMqkUEfmpEhZO0Rn4bXfg9NLw10lUREBkwigz4IjMBgbc0nISiD+rsHukoiIgMmkUEP0Tj99tShcPx58Oy/Q6vmvRGRwSmxQR8GFn1n7ElXQOt2eO7nA10lEZEBkdigTwVGOutw1DQ4cgqsWABvPjfQ1RIR2ecSG/RhaGSyDmbwmZ9EX0by00/BOw0DXTURkX0qsUGf79EDHHI0XPwwuMN9c2DrG3ssKyKSJAkO+oCm5laie7mAmmPh4l9AazP86DRY93AU/CIiCZfYoJ898Qh+98Jb3Prr9bvDftQkuGw5DB8F//cSWHoRbPvbwFZURKSfJTbov332BC49eQx3Pf0aN//iuWi8HuCIE+C/Pw4f/z688jj8nzpY8V3YuXVA6ysi0l/M98Phi7q6Oq+vr9/r/bg7P/zdS/zvxxs4YfRwzpo4iv824XD+oaYaM4N3X4PH/wmefxAqD4J/vAamXBz1+EVEDiBmttrd64quS3LQ5/x81Rvc/+fXWdu4DYBRB1VywuiDOOHIgzimpopRLQ188Ll/5eDGx3EL4IMfx044H0YeC4eMgaGH9lldRET6w6AP+pw3t7Xwu/VvUb/xXZ772zZee+f9dudjx9hmLghXckG4kiPs7/nl28JDefaQT/D8EZ9i10FjqRlWweHDKzlsWAUHDSlj+JAyqitShPHUC9E0/CIi+46CvgvNLW1s2tpCc0sbzS1ptu1s4933d7H1/Z0E77xE2fbXGfr+Xxm3cy3TM/WkyLI6O46/+Ui2ejXbqKLVy9hFihbK2erDeIfhvOvDaU1Vky0bBhVVDKmoYGh5SFVFisAMJxpWKg8DKstDhpaFlKcCysKAVGAEgeHuuEd3+FakAirKQspCy/9iMoPyMKAsLhdY7pdM+zYGZoSBEZq1W2dm8bro4qPC/UZzBUVlgiDaB0A2rlPu39wnx3LlcscKDAPSWSeTdbLxzg3Lb5cKo2Nk3UlnnHafww5tKCyX+0UaGPH7CB2/2TJXt2IfbbPdu4+aFe272H46V6jYegOcrEM26/m6dffLPtced++irl0di05lcm0qPHb0M4rqtfs9a1+eosftXM/cvkvpvnRd686fzXyZ+DOVO43W8f3L1Tebjd/n+POX2zYVGiFZyGbIWEjGA9yjn0Wu/kXf42yGMLODVHonQXYXGUvFjzKyFuJBCg9ShGQJPUNIG5ZJQzaNZdswz2DZNAFZMpYiHVSQDipwC6N6mxECAU5IhoylaLNyslZGFsPZ3Y4wiOpakQr48NG9G0HYU9CXNB99Ug2rLOO4I8q6WDuh/cvtm+HZ+5m64VEmvf8m7Pw7Yes2rMtwALLATsjsDMgQkiEkawFZCh9G1o1sXKTww+jx3g0nyG8RLS/8l/w2XrQ+u/dD/Cx6lcXIEsQ1aV82F82FewniR/t95/ZvHcp0rk9uu65Yt1sQ/wfJbeUURrDH72fH9yJXr+LvTtdfeLynmuTeO/doG7OCdnrx43RsXcetuv7ZdV7TVd2KLS9clnve1ee2WP2swzuY+xwFeL7d0Tvf8XPoZAhJE5J1I7QsZfH/hELZ/Cd89/vjkP+8WcFnP6rH7uOUkaaCNsqs/T7bPDpumjD/mbC4nimyhGRIWfv97ku7PCRNKl+/gCwhWf5uB8GCl/v8eIM66Htk+CiYeRM286bdb5o7ZNOQboV0C+zYAu83RY/W5vwjzLQRZqOeANkMeCb+Nwt49LyTXJfZcQuingxR19MMstlsvieUuwM499/QCv6zuGfjvw7iqLdcVDthNot7BgggCNn9ayDq4kc9Y8ezDkFAxgKiHnAu3IjakKunZ/N/HTgQBEHcq4p6L+ZxfYh7nEBArtcV/0qz6L9xYfzm6uueO55H9c5ta4Z59KsLz0ShYUG+C2nk/mTZ3ZPFvXMo5nqtnvtlmPv1WPgsehV4/Esvm9nd+7QoUNq937ld5/bnhW0irn/+WZGgjd+3gmNbwfr2bcgd1/OfAaOw996hxZ262I65xz+DuLpm7RrvBPn3Pfez8vznynCDLGF+H+ZpwmyG0DO4pWgLUuyyMH9s9+iXpHmWwLO5VoA7Wdv9mYve2ziwzXCLPls7rYx0WEHaysHiTotn8r1t8zbIZiH3ebAg6qlbiAdlZMIhpFNDyAblBJ4h9DYs2xbtI5vGsmmyQRj9eojLeJCCoIxsEOKEZC0k9DZSmRaCbGvUhfL4LxHL/RKL6hZmWwkzLQTZNgJvi+uYJWspshZg5cPpDwr6vWEGYVn0qKiGqpFQc1zfHwYI44eISE8l9jp6ERGJlBT0ZnammW0wswYzm19kvZnZonj9WjObWmpZERHpX90GvZmFwO3AbKIzlHPNrMOZSmYD4+LHlcCdPSgrIiL9qJQe/TSgwd1fdfddwFJgTodt5gD3eeRPwMFmNqrEsiIi0o9KCfrRQOG8vo3xslK2KaUsAGZ2pZnVm1l9U1NTCdUSEZFSlBL0XV1mXMo2pZSNFrovcfc6d6+rqakpoVoiIlKKUi6vbASOKnhdC2wqcZvyEsqKiEg/KqVHvwoYZ2ZjzawcuBBY1mGbZcC8+Oqb6cA2d99cYlkREelH3fbo3T1tZtcAjxLds3OPu68zs6vi9YuB5cBZQAOwA7hsT2W7O+bq1avfMbPXe9mmkcA7vSx7oBqMbYbB2e7B2GYYnO3uaZuP7mrFfjmp2d4ws/quJvZJqsHYZhic7R6MbYbB2e6+bLPujBURSTgFvYhIwiUx6JcMdAUGwGBsMwzOdg/GNsPgbHeftTlxY/QiItJeEnv0IiJSQEEvIpJwiQn6wTIdspkdZWZPmNl6M1tnZtfFyw81s9+Z2cvxv4cMdF37mpmFZvaMmT0Svx4MbT7YzB40sxfjn/k/Jr3dZnZD/Nl+3sx+ZmaVSWyzmd1jZm+b2fMFy7psp5ndHOfbBjP7RE+OlYigH2TTIaeBr7j7h4DpwNVxW+cDj7n7OOCx+HXSXAesL3g9GNr8b8Bv3X08MImo/Yltt5mNBq4F6tz9BKIbLS8kmW3+MXBmh2VF2xn/H78QOD4uc0eceyVJRNAziKZDdvfN7r4mft5M9B9/NFF7fxJv9hPgvAGpYD8xs1rgbOCugsVJb/NwYCZwN4C773L3rSS83UR37A8xsxQwlGh+rMS12d1XAu92WNxVO+cAS9291d1fI5qFYFqpx0pK0Jc8HXKSmNkYYArwZ+DweH4h4n8PG8Cq9YfbgK8B2YJlSW/zMUATcG88ZHWXmVWR4Ha7+9+AhcBfgc1E82b9JwlucwddtXOvMi4pQV/ydMhJYWbVwEPA9e6+faDr05/M7BzgbXdfPdB12cdSwFTgTnefArxPMoYsuhSPSc8BxgJHAlVm9rmBrdV+Ya8yLilBX8pUyolhZmVEIf+Au/8iXvxW/K1exP++PVD16wczgHPNbCPRsNxpZnY/yW4zRJ/rRnf/c/z6QaLgT3K7zwBec/cmd28DfgGcTLLbXKirdu5VxiUl6AfNdMhmZkRjtuvd/YcFq5YBl8TPLwH+376uW39x95vdvdbdxxD9bB9398+R4DYDuPubwBtmdly86HTgBZLd7r8C081saPxZP53oPFSS21yoq3YuAy40swozG0v0/dx/KXmv7p6IB9E0yS8BrwDfHOj69GM7TyH6k20t8Gz8OAsYQXSW/uX430MHuq791P5ZwCPx88S3GZgM1Mc/74eBQ5LebuC7wIvA88BPgYokthn4GdF5iDaiHvvn99RO4Jtxvm0AZvfkWJoCQUQk4ZIydCMiIl1Q0IuIJJyCXkQk4RT0IiIJp6AXEUk4Bb2ISMIp6EVEEu7/A2/7FFZj8MUXAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "input_dim = len(X_train.columns)\n",
    "output_dim = 5\n",
    "hidden_dim = 64\n",
    "layer_dim = 3\n",
    "batch_size = 64\n",
    "dropout = 0.2\n",
    "n_epochs = 100\n",
    "learning_rate = 1e-3\n",
    "weight_decay = 1e-6\n",
    "\n",
    "model_params = {'input_dim': input_dim,\n",
    "                'hidden_dim' : hidden_dim,\n",
    "                'layer_dim' : layer_dim,\n",
    "                'output_dim' : output_dim,\n",
    "                'dropout_prob' : dropout}\n",
    "\n",
    "model = get_model('lstm', model_params)\n",
    "\n",
    "loss_fn = nn.MSELoss(reduction=\"mean\")\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "opt = Optimization(model=model, loss_fn=loss_fn, optimizer=optimizer)\n",
    "opt.train(train_loader, val_loader, batch_size=batch_size, n_epochs=n_epochs, n_features=input_dim)\n",
    "opt.plot_losses()\n",
    "\n",
    "lstm_predictions, values = opt.evaluate(test_loader_one, batch_size=1, n_features=input_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "reg = xgb.XGBRegressor(\n",
    "    tree_method=\"hist\", \n",
    "    n_estimators=64)\n",
    "    \n",
    "clf = reg.fit(X_train_arr, y_train_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "xg_predictions = reg.predict(X_test_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.5851296 , 0.41117203, 0.47383857, 0.51966786, 0.47221434],\n",
       "       [0.60827315, 0.37263748, 0.4183382 , 0.59554946, 0.40028507],\n",
       "       [0.5982437 , 0.3779243 , 0.43675932, 0.58577263, 0.42589524],\n",
       "       ...,\n",
       "       [0.6800196 , 0.5518358 , 0.48310953, 0.6349142 , 0.52033705],\n",
       "       [0.6728498 , 0.5437957 , 0.48335737, 0.6618716 , 0.5423358 ],\n",
       "       [0.6658538 , 0.5452222 , 0.4622061 , 0.6377128 , 0.5501155 ]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xg_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch import nn, Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #TESTING \n",
    "# transformer_model = nn.Transformer(nhead=16, num_encoder_layers=12)\n",
    "\n",
    "# src = torch.rand((32, 512, ))\n",
    "# tgt = torch.rand((32, 512))\n",
    "# out = transformer_model(src, tgt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2409, 39), (803, 5))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_arr.shape, y_test_arr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "\n",
    "\n",
    "def get_src_trg(\n",
    "        sequence: torch.Tensor, \n",
    "        enc_seq_len: int, \n",
    "        target_seq_len: int\n",
    "        ) -> Tuple[torch.tensor, torch.tensor, torch.tensor]:\n",
    "\n",
    "        \"\"\"\n",
    "        Generate the src (encoder input), trg (decoder input) and trg_y (the target)\n",
    "        sequences from a sequence. \n",
    "        Args:\n",
    "            sequence: tensor, a 1D tensor of length n where \n",
    "                    n = encoder input length + target sequence length  \n",
    "            enc_seq_len: int, the desired length of the input to the transformer encoder\n",
    "            target_seq_len: int, the desired length of the target sequence (the \n",
    "                            one against which the model output is compared)\n",
    "        Return: \n",
    "            src: tensor, 1D, used as input to the transformer model\n",
    "            trg: tensor, 1D, used as input to the transformer model\n",
    "            trg_y: tensor, 1D, the target sequence against which the model output\n",
    "                is compared when computing loss. \n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "        assert len(sequence) == enc_seq_len + target_seq_len, \"Sequence length does not equal (input length + target length)\"\n",
    "        \n",
    "        #print(\"From data.TransformerDataset.get_src_trg: sequence shape: {}\".format(sequence.shape))\n",
    "\n",
    "        # encoder input\n",
    "        src = sequence[:enc_seq_len] \n",
    "        \n",
    "        # decoder input. As per the paper, it must have the same dimension as the \n",
    "        # target sequence, and it must contain the last value of src, and all\n",
    "        # values of trg_y except the last (i.e. it must be shifted right by 1)\n",
    "        trg = sequence[enc_seq_len-1:len(sequence)-1]\n",
    "\n",
    "        # trg = trg[:, 0] \n",
    "\n",
    "        if len(trg.shape) == 1:\n",
    "            trg = trg.unsqueeze(-1)\n",
    "        \n",
    "        assert len(trg) == target_seq_len, \"Length of trg does not match target sequence length\"\n",
    "\n",
    "        # The target sequence against which the model output will be compared to compute loss\n",
    "        trg_y = sequence[-target_seq_len:,-5:]\n",
    "\n",
    "        #print(\"From data.TransformerDataset.get_src_trg: trg_y shape before slice: {}\".format(trg_y.shape))\n",
    "\n",
    "        # We only want trg_y to consist of the target variable not any potential exogenous variables\n",
    "        # trg_y = trg_y[:, 0]\n",
    "\n",
    "        assert len(trg_y) == target_seq_len, \"Length of trg_y does not match target sequence length\"\n",
    "\n",
    "        return src, trg, trg_y # change size from [batch_size, target_seq_len, num_features] to [batch_size, target_seq_len] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2409, 39), (803, 39))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_arr.shape, X_test_arr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = np.hstack((X_train_arr, y_train_arr))\n",
    "test_data = np.hstack((X_test_arr, y_test_arr))\n",
    "\n",
    "sequence = np.vstack((train_data, test_data))\n",
    "sequence = torch.tensor(sequence, dtype=torch.float32)\n",
    "\n",
    "enc_seq_len = len(y_train_arr)\n",
    "target_seq_len = len(y_test_arr)\n",
    "\n",
    "src, trg, trg_y = get_src_trg(sequence, enc_seq_len, target_seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2409, 44]), torch.Size([803, 44]), torch.Size([803, 5]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src.shape, trg.shape, trg_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(44, 44)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src.size(-1), trg.size(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_model = nn.Transformer(d_model = 44, nhead = 22, num_encoder_layers=12)\n",
    "\n",
    "out = transformer_model(src, trg)\n",
    "transformer_predictions = out.detach().numpy()[:, -5:] # we just need the final 5 cols for the returns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformer_model = nn.Transformer(d_model = 44, nhead=11, num_encoder_layers=12)\n",
    "# src = torch.rand((2409, 44))\n",
    "# tgt = torch.rand((803, 44))\n",
    "# out = transformer_model(src, tgt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temporal Fusion Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_forecasting import Baseline, TemporalFusionTransformer, TimeSeriesDataSet\n",
    "from pytorch_forecasting.data import GroupNormalizer\n",
    "from pytorch_forecasting.metrics import SMAPE, PoissonLoss, QuantileLoss\n",
    "from pytorch_forecasting.models.temporal_fusion_transformer.tuning import optimize_hyperparameters\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import EarlyStopping, LearningRateMonitor\n",
    "from pytorch_lightning.loggers import TensorBoardLogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-0ce90927e363>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;31m# add additional features\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"month\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdate\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmonth\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"category\"\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# categories have be strings\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"log_volume\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvolume\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1e-8\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"avg_volume_by_sku\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"time_idx\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"sku\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobserved\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvolume\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"mean\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"avg_volume_by_agency\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"time_idx\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"agency\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobserved\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvolume\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"mean\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "from pytorch_forecasting.data.examples import get_stallion_data\n",
    "\n",
    "data = get_stallion_data()\n",
    "\n",
    "# add time index\n",
    "data[\"time_idx\"] = data[\"date\"].dt.year * 12 + data[\"date\"].dt.month\n",
    "data[\"time_idx\"] -= data[\"time_idx\"].min()\n",
    "\n",
    "# add additional features\n",
    "data[\"month\"] = data.date.dt.month.astype(str).astype(\"category\")  # categories have be strings\n",
    "data[\"log_volume\"] = np.log(data.volume + 1e-8)\n",
    "data[\"avg_volume_by_sku\"] = data.groupby([\"time_idx\", \"sku\"], observed=True).volume.transform(\"mean\")\n",
    "data[\"avg_volume_by_agency\"] = data.groupby([\"time_idx\", \"agency\"], observed=True).volume.transform(\"mean\")\n",
    "\n",
    "# we want to encode special days as one variable and thus need to first reverse one-hot encoding\n",
    "special_days = [\n",
    "    \"easter_day\",\n",
    "    \"good_friday\",\n",
    "    \"new_year\",\n",
    "    \"christmas\",\n",
    "    \"labor_day\",\n",
    "    \"independence_day\",\n",
    "    \"revolution_day_memorial\",\n",
    "    \"regional_games\",\n",
    "    \"fifa_u_17_world_cup\",\n",
    "    \"football_gold_cup\",\n",
    "    \"beer_capital\",\n",
    "    \"music_fest\",\n",
    "]\n",
    "data[special_days] = data[special_days].apply(lambda x: x.map({0: \"-\", 1: x.name})).astype(\"category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['agency', 'sku', 'volume', 'date', 'industry_volume', 'soda_volume',\n",
       "       'avg_max_temp', 'price_regular', 'price_actual', 'discount',\n",
       "       'avg_population_2017', 'avg_yearly_household_income_2017', 'easter_day',\n",
       "       'good_friday', 'new_year', 'christmas', 'labor_day', 'independence_day',\n",
       "       'revolution_day_memorial', 'regional_games', 'fifa_u_17_world_cup',\n",
       "       'football_gold_cup', 'beer_capital', 'music_fest',\n",
       "       'discount_in_percent', 'timeseries', 'time_idx', 'month', 'log_volume',\n",
       "       'avg_volume_by_sku', 'avg_volume_by_agency'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_prediction_length = 6\n",
    "max_encoder_length = 24\n",
    "training_cutoff = df[\"time_idx\"].max() - max_prediction_length\n",
    "\n",
    "training = TimeSeriesDataSet(\n",
    "    df[lambda x: x.time_idx <= training_cutoff],\n",
    "    time_idx=\"time_idx\",\n",
    "    target=\"volume\",\n",
    "    group_ids=[\"agency\", \"sku\"],\n",
    "    min_encoder_length=max_encoder_length // 2,  # keep encoder length long (as it is in the validation set)\n",
    "    max_encoder_length=max_encoder_length,\n",
    "    min_prediction_length=1,\n",
    "    max_prediction_length=max_prediction_length,\n",
    "    static_categoricals=[\"agency\", \"sku\"],\n",
    "    static_reals=[\"avg_population_2017\", \"avg_yearly_household_income_2017\"],\n",
    "    time_varying_known_categoricals=[\"special_days\", \"month\"],\n",
    "    variable_groups={\"special_days\": special_days},  # group of categorical variables can be treated as one variable\n",
    "    time_varying_known_reals=[\"time_idx\", \"price_regular\", \"discount_in_percent\"],\n",
    "    time_varying_unknown_categoricals=[],\n",
    "    time_varying_unknown_reals=[\n",
    "        \"volume\",\n",
    "        \"log_volume\",\n",
    "        \"industry_volume\",\n",
    "        \"soda_volume\",\n",
    "        \"avg_max_temp\",\n",
    "        \"avg_volume_by_agency\",\n",
    "        \"avg_volume_by_sku\",\n",
    "    ],\n",
    "    target_normalizer=GroupNormalizer(\n",
    "        groups=[\"agency\", \"sku\"], transformation=\"softplus\"\n",
    "    ),  # use softplus and normalize by group\n",
    "    add_relative_time_idx=True,\n",
    "    add_target_scales=True,\n",
    "    add_encoder_length=True,\n",
    ")\n",
    "\n",
    "# create validation set (predict=True) which means to predict the last max_prediction_length points in time\n",
    "# for each series\n",
    "validation = TimeSeriesDataSet.from_dataset(training, data, predict=True, stop_randomization=True)\n",
    "\n",
    "# create dataloaders for model\n",
    "batch_size = 128  # set this between 32 to 128\n",
    "train_dataloader = training.to_dataloader(train=True, batch_size=batch_size, num_workers=0)\n",
    "val_dataloader = validation.to_dataloader(train=False, batch_size=batch_size * 10, num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def depict_results(predictions):\n",
    "    \n",
    "    arr = []\n",
    "    for prediction in predictions:\n",
    "        arr.append(prediction.flatten())\n",
    "\n",
    "    predictions = np.array(arr)\n",
    "    predicted = scaler.inverse_transform(predictions) \n",
    "\n",
    "    df_predicted = pd.DataFrame(predicted, \n",
    "                columns=['Brazil Returns',\n",
    "                'Russia Returns',\n",
    "                'India Returns', \n",
    "                'China Returns',\n",
    "                'South Africa Returns'])\n",
    "\n",
    "    return df_predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions\n",
    "lstm_df = depict_results(lstm_predictions)\n",
    "xg_df = depict_results(xg_predictions)\n",
    "transformer_df = depict_results(transformer_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lstm_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-615fd4ceffa1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mlstm_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'lstm_results.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mxg_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'xg_results.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mtransformer_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'transformer_results.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'lstm_df' is not defined"
     ]
    }
   ],
   "source": [
    "lstm_df.to_csv('lstm_results.csv')\n",
    "xg_df.to_csv('xg_results.csv')\n",
    "transformer_df.to_csv('transformer_results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lstm_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-208bf71ec7c0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mlstm_df\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'lstm_df' is not defined"
     ]
    }
   ],
   "source": [
    "lstm_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_df.drop(columns='Unnamed: 0', inplace=True)\n",
    "xg_df.drop(columns='Unnamed: 0', inplace=True)\n",
    "transformer_df.drop(columns='Unnamed: 0', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_mae = mean_absolute_error(lstm_df, y_test)\n",
    "xg_mae = mean_absolute_error(xg_df, y_test)\n",
    "transformer_mae = mean_absolute_error(transformer_df, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.012295575133412848, 0.01926163054190369, 0.08135306997088222)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_mae, xg_mae, transformer_mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_mse = mean_squared_error(lstm_df, y_test)\n",
    "xg_mse = mean_squared_error(xg_df, y_test)\n",
    "transformer_mse = mean_squared_error(transformer_df, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0005884616188611248, 0.000937936838406417, 0.011095821232349866)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_mse, xg_mse, transformer_mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_results_arr = lstm_df.to_numpy().flatten()\n",
    "y_test_arr = y_test_arr.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4015,), (4015,))"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_arr.shape, lstm_results_arr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_drawdown(df, window=252):\n",
    "    \n",
    "    roll_max = df['Close'].rolling(window, min_periods=1).max()\n",
    "    daily_drawdown = df['Close']/roll_max - 1.0\n",
    "\n",
    "    max_daily_drawdown = daily_drawdown.rolling(window, min_periods=1).min()\n",
    "\n",
    "    return max_daily_drawdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aa46f9d99ecb56874a909196cc8fe851fdad884172da2927a3a528c17ec588e0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
